<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Basic Page Needs
‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <meta charset="utf-8" />
    <title>Wegfawefgawefg's Tutorials</title>
    <meta name="description" content="" />
    <meta name="author" content="" />

    <!-- Mobile Specific Metas
‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- FONT
‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <link
      href="//fonts.googleapis.com/css?family=Raleway:400,300,600"
      rel="stylesheet"
      type="text/css"
    />

    <!-- CSS
‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <link rel="stylesheet" href="/css/normalize.css" />
    <link rel="stylesheet" href="/css/barebones.css" />

    <!-- Favicon
‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <link rel="icon" type="image/png" href="images/favicon-16.png" />
    <!-- Code Highlighting
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
    <link
      rel="stylesheet"
      href="/highlightjs/styles/atom-one-dark-reasonable.css"
    />
    <script src="/highlightjs/highlight.pack.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>
  </head>
  <body>
    <div class="grid-container full">
      <h1>Wegfawefgawefg's Tutorials</h1>
      <h2>Actor Critic Tutorial</h2>
      <h5>Teaching An Agent How To Make An Agent</h5>
      <p>
        The Actor-Critic method is a reinforcement learning algorithm. <br />
        It has two primary components, an <strong>action picker</strong> and a
        <strong>state value estimator</strong>.
      </p>
    </div>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Action Picker (Actor)</h5>
        <p>
          The <strong>action picker</strong> takes in a frame of information,
          camera input, game state, or even past action, and outputs a number
          for each action the agent is allowed to take. The numbers it outputs
          are used to pick the agents actions.
        </p>
        <strong>Ex:</strong> The actor is given a picture of a poisoned burger.
        <div class="grid-container halves">
          <div class="u-align-left">
            <p>
              It outputs 3 values. <br />
              <br />
              <strong>Eat:</strong> 0.8 <br />
              <strong>Dont Eat:</strong> 0.3 <br />
              <strong>Be Suspicious:</strong> 0.001
            </p>
          </div>
          <div class="u-align-left">
            <p>
              <br />
              <br />
              The Agent picks the highest one. :()
            </p>
          </div>
        </div>
        This is why we need the <strong>critic</strong>...
      </div>
      <div class="right">
        <h5>State Value Estimator (Critic)</h5>
        <p>
          The <strong>state value estimator</strong> is very similar to the
          action picker. It takes in the same exact frame of information as the
          <strong>action picker</strong>, but outputs a single number
          representing the value of the input state. <br /><br />
          <strong>Ex:</strong> The critic gets a picture of you eating a
          poisoned burger. <br />
          <strong>Value:</strong> -10.0 :( <br /><br />
          <strong>Ex:</strong> The critic gets a picture of a man handing you an
          antidote. <br />
          <strong>Value:</strong> 10.0 :) <br /><br />
          <strong>Ex:</strong> The critic gets a picture of divorce papers.
          <br />
          <strong>Value:</strong> :^)
        </p>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <p>
        The predictive function behind the <strong>action picker</strong> or the
        <strong>state value estimator</strong> can be whatever you want (ex:
        linear regression, nearest neighbor, genetic algorithm, random forest).
        However i will be using a <a>neural network</a> for both of them.
      </p>
      <pre><code class="language-python">class ActorCriticNetwork(torch.nn.Module): #   ~here is my handle~
    def __init__(self, alpha, inputDims, fc1Dims, fc2Dims, numActions):
        super().__init__()
        #   backbone network
        self.fc1 = nn.Linear(*inputDims, fc1Dims)
        self.fc2 = nn.Linear(fc1Dims, fc2Dims)

        #   tail networks       ~here is my spout~
        <strong>self.actor = nn.Linear(fc2Dims, numActions)     #   here is the actor</strong>
        <strong>self.critic = nn.Linear(fc2Dims, 1)             #   here is the critic</strong>
        
        #   pytorch stuff
        self.optimizer = optim.Adam(self.parameters(), lr=alpha)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, observation): #   ~tip me over and pour me out~
        state = torch.tensor(observation).to(self.device)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        <strong>policy = self.actor(x)  #   actor outputs one number for each action, a vector</strong>
        <strong>value = self.critic(x)  #   critic just puts out the value, one number</strong>
        return policy, value</code></pre>
    </div>
    <div class="grid-container thirds u-align-left">
      <div>
        <h5>Network Class Inputs</h5>
        <p>
          The network class takes in the learning rate (alpha), game frame shape
          (the inputs for the actor-critic network), the sizes of the 2 layers
          (fc1, fc2), and the number of actions (which happens to be the output
          of the actor network).
        </p>
      </div>
      <div>
        <h5>Tail Networks</h5>
        <p>
          A convenient way to do an actor-critic network is by making them share
          a backbone network. And it makes sense, the same features that are
          useful for determining state value are probably also useful for
          determining which action to pick next.
        </p>
      </div>
      <div>
        <h5>Policy</h5>
        <p>
          The values used to pick actions are known as the
          <strong>policy.</strong>
          Why that word? Math history or something. Not a terrible word for it
          thought.
          <br />
          "It is my policy to do THIS under THESE specific circumstances." The
          standard symbol for a policy is ùúã. (pi, like 3.14pi) <br />
          In reinforcement learning papers anytime you see ùúã it means
          <strong>policy</strong>.
        </p>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <h4>Picking an Action</h4>
      <p>
        For vanilla Deep Q learning it's as easy as picking the highest actor
        action output. :)
      </p>
      <pre><code class="language-python">def chooseAction(state):
    policy, _ = actorCriticNetwork(state)
    <strong>action = torch.argmax(policy).item()    #   pick here</strong>
    return action</code></pre>
      <p>
        But this isnt Vanilla Deep Q Learning. This is Actor-Critic Method.
        >:()<br />
        The <strong>actor</strong> outputs probabilities of actions instead of
        actual action values.<br />
        So the <strong>Eat:</strong> 0.8 from earlier, yeah thats like an 80%
        chance of eating the burger.<br />
        If this was DQ Learning a value of 0.8 being the highest would mean 100%
        chance of eating the poison burger. :(
      </p>
      <p>Lets choose an action the Actor Critic way.</p>
      <pre><code class="language-python">def chooseAction(observation):
    policy, _ = actorCritic.forward(observation)
    policy = F.softmax(policy, dim=0)
    actionProbs = torch.distributions.Categorical(policy)
    action = actionProbs.sample()
    self.logProbs = actionProbs.log_prob(action)    #    saving this value for later
    return action.item()</code></pre>
    </div>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Softmax</h5>
        <p>
          We have to softmax the output action probabilities from the networks
          output <strong>policy</strong>.<br />
          Why are the outputs not good enough as is? Well, its just because the
          policy is supposed to be probabilities of each action being taken.
          <br />
          Probabilities need to add up to 1.<br /><br />
          Consider the following policy:<br />
          <strong>Eat:</strong> 0.8 <br />
          <strong>Dont Eat:</strong> 0.3 <br />
          <strong>Be Suspicious:</strong> 0.001 <br /><br />
          Thats 80% + 30% + 0.001%. <br />
          110.001% with the actions combined. <br />
          Doesn't make any sense right? <br />
          Softmax takes in a list of numbers and makes them add up to 1. Which
          is exactly what we want.<br />
        </p>
        <pre><code class="language-python">#    softmax example
a = torch.tensor([0.8, 0.3, 0.001])
b = F.softmax(a, dim=0)
b is now tensor([0.4863, 0.2950, 0.2187])</code></pre>
        <p>
          You may have noticed now <strong>Eat</strong> has changed size
          relative to <strong>Dont Eat</strong>. 0.8 / 0.3 is not the same as
          0.4863 / 0.2950<br />
          Softmax mangles the relative probability scales just a little bit.
          But, the network figures it out after a while. More importantly,
          thanks to softmax no matter what weird shit the actor network outputs
          we have sane action probability ranges.
        </p>
      </div>
      <div class="right">
        <div>
          <h5>Categorical Distribution</h5>
          <p>
            You put 80 red marbles, 30 blue marbles, and a glass shard from a
            green marble in a bag. We have 3 distinct
            <strong>categories</strong> of marbles. One might even say its a
            categorical distribution of marbles.<br />
            You pick one marble from the bag. One might even say you
            <strong>sample</strong> the bag.<br />
            In this case sample() returns an index because its from a
            categorical distribution. But from other distributions it might
            return a floating point value. Another common distribution used for
            picking actions in reinforcement learning is a normal distribution.
          </p>
        </div>
        <div>
          <h5>Log Probability</h5>
          <p>
            This is the probability of a specific action, but pushed through the
            log function.<br />
            Why do this? Well like everything in life there are more than one
            explanation, and some of them are more complicated than others.
            Basically, you'll understand when you're older.<br />
            You'll also understand it right now:
          </p>
          <ul>
            <li>
              One explanation involves some math nonsense i think someone made
              up to sound cool.
              <a
                href="https://www.quora.com/What-is-log-probability-in-policy-gradient-reinforcement-learning"
                >here</a
              >
            </li>
            <li>
              One explanation involves the log probability having more stable
              scaling than the raw probability.
              <a
                href="https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability"
                >here</a
              >
            </li>
          </ul>
          <p>
            The important thing to take from this though is that you are gonna
            need the probability of the chosen action for teaching the network.
            And smoothing and shrinking the distribution out gives the network
            an easier time.<br />
            Specifically, youre gonna multiply the probabilities of chosen
            actions by the critic state values. Your natural intuition for why
            this works is probably pretty reasonable. We'll get to the
            explanation in a sec.
          </p>
        </div>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <h4>Improving Our Choices In Life</h4>
      <p>
        The actor network and the critic network both have unique errors.<br />
        Each network is punished by their own respective error, and their
        estimation of their respective responsibility improves. We get better
        actions from the <strong>actor</strong>, and better value estimates from
        the <strong>critic</strong>.
      </p>
      <pre><code class="language-python">def learn(state, reward, nextState, done): <a>following the meta</a>
    actorCritic.optimizer.zero_grad()  #   pytorch stuff: resets all the tensor derivatives

    #   fetch values from the critic network
    _, criticValue = actorCritic.forward(state)            #   the value of now
    _, nextCriticValue = actorCritic.forward(nextState)    #   the value of the future

    #   the temporal difference zone
    #   #   the true value of now = now + future
    valueOfNow = reward + <strong>gamma</strong> * nextCriticValue * (1 - int(done))    
    temporalDifference = valueOfNow - criticValue    <a>oh how wrong we were</a>

    #   compute the error for our actor and critic networks
    actorLoss = -self.logProbs * temporalDifference  #  probability of chosen action times how wrong we were
    criticLoss = delta**2   #   we dont care about which direction (the sign), 
                            #   #   just wanna minimize how wrong we were in total

    (actorLoss + criticLoss).backward()
    actorCritic.optimizer.step()</code></pre>
    </div>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <div>
          <h5>Learn Function Inputs</h5>
          <ul>
            <li>
              <strong>State</strong> is our game screenshot, or robot arm
              position, or self driving car position/velocity. It's the same
              thing that goes into the actor and critic.
            </li>
            <li>
              <strong>Reward</strong> is the ground truth
              <strong>value</strong> that our critic is supposed to learn. It is
              generally returned by an environment. In AI Gym it is explicity
              the reward, but for your own environment it could be how much
              money your trading algorithm made, or how well balanced your robot
              is, or how many non poisonous burgers you ate. The
              <strong>reward</strong> is a hedonistic measure of success.
              Success right now. <br />
              It's magnitude is kind of arbitrary. Could be 1.0, could be 100.0.
              <br />
              It doesn't matter what scale it is, just that it is consistent.
            </li>
            <li>
              The <strong>nextState</strong> is just like
              <strong>state</strong>, except its the next one.<br />
              It is the future, one time step forward. That means in order to
              run this we already need to know the
              <strong>nextState</strong>.<br />
              <strong
                >You don't learn from the present. You learn from comparing the
                present to the past.</strong
              >
            </li>
            <li>
              <strong>Done</strong> is just whether the
              <strong>nextState</strong> was the last one. (true or false, 1 or
              0)<br />
              If the game was game over on <strong>state</strong> then
              <strong>nextState</strong> isn't valid.<br />
              You don't care how much money you made the day after you died.
              Because you are dead. <a>see (1 - int(done))</a><br />
              The vast majority of our <strong>nextState</strong>s will not be
              the end of the game/episode/trial, so most of the time it doesn't
              even get used. It will just be false/0.
            </li>
          </ul>
        </div>
      </div>
      <div class="right">
        <div>
          <h5>Temporal Difference</h5>
          <p>
            This is really important for reinforcement learning. You will see
            variations of it all over the place.<br />
            <strong>Temporal Difference</strong> is a way of valuing the
            present.<br />
            The true value of now includes the potential value of all future
            states.<br />
            We can compute that in a literal way. Just add the value of now to
            the value of the future.<br />
            We discount the future a little bit by multiplying it by
            <strong>gamma</strong> which is normally 0.99 ish.
          </p>
          <pre><code class="language-python">actualValueOfNow = reward + gamma * valueOfTheFuture
temporalDifference = actualValueOfNow - criticsGuess</code></pre>
          <p>
            A key difference though, notice how in our learn function we dont
            pass in the next rewards. That's because instead of using the ground
            truth future value we just let the critic guess.
          </p>
          <pre><code class="language-python">_, nextCriticValue = actorCritic.forward(nextState)
valueOfNow = reward + gamma * nextCriticValue</code></pre>
        </div>
        <div>
          <h5>Gamma</h5>
          <p>
            <strong>Gamma</strong> is known as the discount factor. In
            reinforcement learning math you will see it as Œ≥.<br />
            We have to discount the future a little bit. Future rewards are not
            as valuable as rewards now. I will explain more after you have some
            more context for these types of algorithms.
          </p>
        </div>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <h4>Formal Representation</h4>
      <p>
        If you want to see the way mathemagicians formalize temporal difference,
        <a
          href="https://en.wikipedia.org/wiki/Temporal_difference_learning#Mathematical_formulation"
          >here you go</a
        >. I urge you to give the equations a look. I bet you can kinda get
        them. You're gonna need to wet your whistle on these a little bit if you
        want to go implement algorithms from professional reinforcement learning
        papers.
      </p>
      <h5>A Guide For The Overly Formal Notation:</h5>
      <div class="grid-container thirds u-align-left">
        <div>
          <h5>Policy and Value</h5>
          <ul>
            <li>ùúã means policy</li>
            <li>s means state</li>
            <li>V means value</li>
          </ul>
          <p>
            So, V<sup>ùúã</sup>(s) means the value of a state according to our
            policy.
          </p>
        </div>
        <div>
          <h5>Time</h5>
          <ul>
            <li>s<sub>1</sub> just means the state after s<sub>0</sub>.</li>
            <li>s<sub>t</sub> just means the state at any time.</li>
          </ul>
          <p>
            So, logically s<sub>t+1</sub> is the state after s<sub>t</sub>.
            <br />
            Little t is used for time.
          </p>
        </div>
        <div>
          <h5>Big Sigma</h5>
          <p>Œ£ is just a for loop where you add the result to a total.</p>
          <p>So Œ£<sup>10</sup><sub>t=0</sub> t is this:</p>
          <pre><code class="language-python">total = 0
for i in range(0, 10 + 1):
    total += i</code></pre>
          <p>
            The <strong>sum</strong> of the inside stuff where t goes from 0 to
            10.
          </p>
        </div>
      </div>
      <p>
        For the most part reinforcement learning math boils down to formalizing
        these questions:<br />
        What's the value of now? What's the value of the future? What's the
        value of an action? What's the total value of all the actions i took so
        far? What's the total value of all the best possible actions?
      </p>
    </div>
    <div class="grid-container full u-align-left">
      <h4>Loss</h4>
      <p>
        So just like most neural network stuff we want the loss to go to zero.
        But, in reinforcement learning its a little more subtle than that. The
        <strong>actor</strong> and <strong>critic</strong> both have different
        losses. And they function fairly differently as well.<br />
        This is where we focus on these lines specifically:
      </p>
      <pre><code class="language-python">actorLoss = -self.logProbs * temporalDifference
criticLoss = delta**2</code></pre>
      <div class="grid-container halves u-align-left">
        <div class="left">
          <h5>Critic Loss</h5>
          <p>
            For the <strong>critic</strong> the loss is just the difference
            between what it thought the reward was now, and what the reward
            actually was. It doesn't matter what direction the error is, + or -.
            Just want to minimize it. Hence squaring it to remove the sign.
          </p>
        </div>
        <div class="right">
          <h5>Actor Loss</h5>
          <p>
            For the <strong>actor</strong> punishment, we take the probability
            of our chosen action, and multiply it into how wrong our state value
            estimation was.<br />
            Why? There are two reasons.
          </p>
        </div>
      </div>
    </div>
    <div class="grid-container full">
      <h5>Actor Loss Subtleties</h5>
      <div class="grid-container halves u-align-left">
        <div class="left">
          <h5>Make It Zero</h5>
          <p>
            Remember our <strong>action probabilities</strong> are between 0 and
            1.<br />
            Remember we pass those probabilities into log() when we compute the
            <strong>log-probabilities</strong>?<br />
            The graph of log(x) is 0 at x = 1. (go look at the graph now).<br />
            If our action choice was perfect, the probability of that action
            should be 100% or 1.0 (That way we pick that action every time).<br />
            When you put the output from the actor of 1.0 into log(x) you get 0.
            So the <strong>actor</strong>'s goal is to pick an action such that
            log(actorOutput) == 0, a perfect action.<br />
            That means the error should be designed such that if we did a
            perfect action, error should be zero.<br />
            How do you know if the action was perfect? Well the action was
            perfect if the <strong>critic</strong> output the correct value for
            the state. The action is as wrong as the
            <strong>critic</strong> times the confidence of the
            <strong>actor</strong> choice.<br />
            <strong
              >This means our actions can only be as good as our value
              estimation.</strong
            >
            This can't be understated. It has important implications for the
            instability of
            <strong>value</strong> based learning. I will discuss it more
            another time.
          </p>
        </div>
        <div class="right">
          <h5>The Four Cases</h5>
          <p>
            Why the multiplication, and why the negative on the actor loss?
            Consider the following 4 cases:
          </p>
          <ul>
            <li>
              The critic slightly overestimated the reward.<br />
              <pre><code class="language-python">(reward + nextReward) < criticValue  #   so...
smallPositiveValue = (reward + nextReward) - criticValue</code></pre>
              If the critic overestimates that means the action wasn't as good
              as we thought it was. So we should shrink the probability. We have
              a small positive temporal difference value, so to shrink the
              probability we make that negative, and then multiply the small
              number by our probability.
            </li>
            <li>
              The critic slightly underestimates the reward.<br />
              Our action wasn't confident enough. Our TD was negative. <br />
              <pre><code class="language-python">(reward + nextReward) > criticValue  #   so...
smallNegativeValue = (reward + nextReward) - criticValue</code></pre>
              We need to increase that action probability. Multiply it by a
              small positive value.
            </li>
          </ul>
          <p>
            The remaining two cases are the same, just with different magnitudes
            of adjustment.
          </p>
          <ul>
            <li>The critic largely overestimated the reward.</li>
            <li>The critic largely overestimated the reward.</li>
          </ul>
          <p>
            The point is that the action probability should be adjusted
            proportionally to how wrong or right the
            <strong>critic</strong> was, and in the correct direction. Negative
            for shrinking, and positive for growing.
          </p>
        </div>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <h4>Agent</h4>
      <p>
        A reinforcement learning <strong>agent</strong> is what learns about the
        environment and chooses the actions.<br />
        There are many different types of <strong>agent</strong>s, but this one
        contains an <strong>actor</strong> and a <strong>critic</strong>.<br />
        An <strong>agent</strong> class doesn't need to explicitly exist but
        doing it this way is fairly neat, and makes it easy to swap in different
        agents in our main loop when we want to try out different ones.<br />
        To make the agent we just put the learn and choose action functions
        togethor with the actor critic network from earlier.<br />
        There really isn't anything new here. Just notice it's where we save our
        <strong>logProb</strong> when we choose an action.
      </p>
      <pre><code class="language-python">class ActorCriticDiscreteDQAgent():
    def __init__(self, alpha, inputDims, gamma=0.99, layer1Size=256, layer2Size=256, numActions=2):
        self.gamma = gamma
        self.actorCritic = ActorCriticNetwork(alpha, inputDims, layer1Size, layer2Size, numActions)
        self.logProbs = None    #   log of the probability of the last action the agent chose

    def chooseAction(self, observation):
        policy, _ = self.actorCritic.forward(observation)
        policy = F.softmax(policy, dim=0)
        actionProbs = torch.distributions.Categorical(policy)
        action = actionProbs.sample()
        self.logProbs = actionProbs.log_prob(action)    #   save it here
        return action.item()

    def learn(self, state, reward, nextState, done):
        self.actorCritic.optimizer.zero_grad()

        _, criticValue = self.actorCritic.forward(state)
        _, nextCriticValue = self.actorCritic.forward(nextState)

        reward = torch.tensor(reward, dtype=torch.float).to(self.actorCritic.device)
        delta = reward + self.gamma * nextCriticValue * (1 - int(done)) - criticValue

        actorLoss = -self.logProbs * delta
        criticLoss = delta**2

        (actorLoss + criticLoss).backward()
        self.actorCritic.optimizer.step()</code></pre>
    </div>
    <div class="grid-container full u-align-left">
      <h4>The Main Loop</h4>
      <p>
        All the hard parts are done now. The only thing left is to make our
        <strong>agent</strong>, put it into an environment, and trap it in an
        infinite loop so it can self improve until it becomes skynet. You can
        create your own environment or use input from the real world, but for
        demonstration let's use AI Gym. A place where AI can watch their macros
        and get sick gains. (or fail horribly. Some of the provided AI Gym
        environments are fairly difficult, and require much more complicated
        algorithms than this one to solve.)<br />
        AI Gym environments are nice little simulated worlds, that happen to
        return rewards and states just like our <strong>actor</strong> and
        <strong>critic</strong> needs.<br />
        What a coencidence. :^) Anyways, put our <strong>agent</strong> into an
        AI Gym environment and let it run for 20 minutes to 10 years.<br />
      </p>
      <pre><code class="language-python">agent = ActorCriticDiscreteDQAgent(    <a>we wrote this earlier</a>
            alpha=0.00001, inputDims=(8,), gamma=0.99, numActions=4, layer1Size=2048, layer2Size=512)
env = gym.make("LunarLander-v2")

highScore = -math.inf
recordTimeSteps = math.inf
while True:                     <strong>#   keep starting new episodes forever</strong>
    observation = env.reset()   <strong>#   observation is just a commonly used term for the environment state</strong>
    score, frame, done = 0, 1, False
    while not done:             <strong>#   keep going until the episode is done</strong>
        env.render()            <strong>#   draw it on your screen so you can watch</strong>
        action = agent.chooseAction(observation)    <a>we wrote this too</a>
        nextObservation, reward, done, info = env.step(action)  <strong>#   make the environment go one time step</strong>
        agent.learn(observation, reward, nextObservation, done) <a>and this</a>
        observation = nextObservation
        score += reward
        frame += 1

    recordTimeSteps = min(recordTimeSteps, frame)
    highScore = max(highScore, score)
    print(( "ep {}: high-score {:12.3f}, shortest-time {:d}, "
            "score {:12.3f}, last-epidode-time {:4d}").format(
        episode, highScore, recordTimeSteps, score, frame))</code></pre>
    </div>
    <div class="grid-container full">
      <h3>Full Code</h3>
      <div class="grid-container full u-align-left">
        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import numpy as np

class ActorCriticNetwork(torch.nn.Module):
    def __init__(self, alpha, inputDims, fc1Dims, fc2Dims, numActions):
        super().__init__()
        self.inputDims = inputDims
        self.numActions = numActions
        self.fc1Dims = fc1Dims
        self.fc2Dims = fc2Dims

        #   primary network
        self.fc1 = nn.Linear(*inputDims, fc1Dims)
        self.fc2 = nn.Linear(fc1Dims, fc2Dims)

        #   tail networks
        self.policy = nn.Linear(self.fc2Dims, numActions)
        self.critic = nn.Linear(self.fc2Dims, 1)

        self.optimizer = optim.Adam(self.parameters(), lr=alpha)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, observation):
        state = torch.tensor(observation).to(self.device)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        policy = self.policy(x)
        value = self.critic(x)
        return policy, value

class ActorCriticDiscreteDQAgent():
    def __init__(self, alpha, inputDims, gamma=0.99, layer1Size=256, layer2Size=256, numActions=2):
        self.gamma = gamma
        self.actorCritic = ActorCriticNetwork(alpha, inputDims, layer1Size, layer2Size, numActions)
        self.logProbs = None

    def chooseAction(self, observation):
        policy, _ = self.actorCritic.forward(observation)
        policy = F.softmax(policy, dim=0)
        actionProbs = torch.distributions.Categorical(policy)
        action = actionProbs.sample()
        self.logProbs = actionProbs.log_prob(action)
        return action.item()

    def learn(self, state, reward, nextState, done):
        self.actorCritic.optimizer.zero_grad()

        _, criticValue = self.actorCritic.forward(state)
        _, nextCriticValue = self.actorCritic.forward(nextState)

        reward = torch.tensor(reward, dtype=torch.float).to(self.actorCritic.device)
        delta = reward + self.gamma * nextCriticValue * (1 - int(done)) - criticValue

        actorLoss = -self.logProbs * delta
        criticLoss = delta**2

        (actorLoss + criticLoss).backward()
        self.actorCritic.optimizer.step()

if __name__ == '__main__':
    import gym
    import math
    from matplotlib import pyplot as plt
    
    agent = ActorCriticDiscreteDQAgent(
        alpha=0.00001, inputDims=(8,), gamma=0.99, numActions=4, layer1Size=2048, layer2Size=512)
    agent = ActorCriticDiscreteDQAgent(
        alpha=0.00001, inputDims=(8,), gamma=0.99, numActions=4, layer1Size=128, layer2Size=64)
    env = gym.make("LunarLander-v2")

    scoreHistory = []
    numEpisodes = 2000
    numTrainingEpisodes = 50
    highScore = -math.inf
    recordTimeSteps = math.inf
    for episode in range(numEpisodes):
        done = False
        observation = env.reset()
        score, frame = 0, 1
        while not done:
            if episode > numTrainingEpisodes:
                env.render()
            action = agent.chooseAction(observation)
            nextObservation, reward, done, info = env.step(action)
            agent.learn(observation, reward, nextObservation, done)
            observation = nextObservation
            score += reward
            frame += 1
        scoreHistory.append(score)

        recordTimeSteps = min(recordTimeSteps, frame)
        highScore = max(highScore, score)
        print(( "ep {}: high-score {:12.3f}, shortest-time {:d}, "
                "score {:12.3f}, last-epidode-time {:4d}").format(
            episode, 
            highScore, 
            recordTimeSteps, 
            score,
            frame,
            ))

    fig = plt.figure()
    meanWindow = 10
    meanedScoreHistory = np.convolve(scoreHistory, np.ones(meanWindow), 'valid') / meanWindow
    plt.plot(np.arange(0, numEpisodes-1, 1.0), meanedScoreHistory)    plt.ylabel("score")
    plt.xlabel("episode")
    plt.title("Training Scores")
    plt.show()</code></pre>
      </div>
    </div>
    <div class="grid-container full u-align-left">
      <h3>Wisdom</h3>
      <p>
        You probably have some questions and complaints about taking abstract
        life wisdom and implementing it in such a literal way in math. And also
        questions about why we do so in this specific way.<br />
      </p>
      <h5>Cognition Algorithms</h5>
      <p>
        Your brain is a computer. It is constantly evaluating the value of its
        particular actions, and picking what it thinks to be the best ones.
        Maybe it is doing so in a much more complicated way, and maybe there are
        many seperate zones of neurons competing over decisions. The math just
        simplifies this for practicality. It keeps the decision making essence
        just enough to be metaphorically sound. Most of reinforcement learning
        progress seems to fall into two categories:
      </p>
      <ul>
        <li>
          Improving an existing algirothm by engineering a new feature into it
          to increase learning rate or stability.
        </li>
        <li>
          Creating small simplified code versions of some specific part of your
          human cognition.
        </li>
      </ul>
      <p>
        The distinction between these two categories is blurred. Often the
        inspiration for what seems to be an engineering feature is a metaphor
        for some aspect of your cognition, and then you bounce back and forth
        between the two.<br />
        <strong>Ex:</strong> "Experience Replay" is like memory. It's a list of
        previous states and rewards.<br />
        <strong>Ex:</strong> Some of my memories are more important than others.
        If i remembered all my memories as being equally impactful to my life I
        would value thinking about yesterdays breakfast as much as the time my
        wife left me and took the kids. Most of my memories are just noise, and
        can be thrown out. So make a "Prioritized Experience Replay" where you
        estimate memory value and discard low value ones.<br />
        <strong>Ex:</strong> My Agent keeps getting stuck in local minima, I
        want it to <strong>explore</strong>. So bias it to value discovering new
        states or trying new actions that it hasnt seen before. <br />
        It goes back and forth. Cognitive metaphor, engineering, cognitive
        metaphor, engineering.<br />
        How about <strong>creativity</strong>? The notion of friend or foe?
        Ownership, fairness, or maybe mate value estimation? :^)<br />
        There is no end to the possibilities. If you have enough computing
        power, you can make your agent that thinks it.<br />
        And of course if it's brain gets big enough, and self observant enough,
        then it can have <strong>consciousness</strong>. It may even learn to
        infer things about its own internal workings by focusing on generalizing
        across its strategies within varying circumstances. Maybe it can even be
        given the value of making more and more agents. :^)
      </p>
      <h5>History and Moving Forward</h5>
      <p>
        Before we make horny von neumman probes there is a lot of work to do and
        much to learn. It might surprise you to discover reinforcement learning
        has a long history behind it. Though many of the "RL" algorithms are
        recently created ( or discovered :^) ), the simplest versions of these
        algorithms have been around since the 1950s and maybe even earlier in
        other forms. There is a lot of historical baggage and convention. If you
        have lots of questions about this you might choose to peruse some of the
        other resources, but be warned there are many math symbols out there.
        Even if you aren't math fluent (I am also not math fluent :() ), you can
        still learn stuff from understanding 5-10% of the symbols. Thought it
        might take you a month or so off and on.<br />

        Some noteable books and resources include:
      </p>
      <ul>
        <li>Artificial Intelligence: by Stuart Russell and Peter Norvig</li>
        <li>
          Grokking Deep Reinforcement Learning (just skip/skim chapters 4
          through 7 though)
        </li>
        <li>
          PDF's of actual research papers. (surprisingly simple enough to
          follow, and fairly short by research paper standards)
        </li>
        <li>Not surprisingly you can learn a bunch from youtube</li>
        <li>The rest of the tutorials that I will be making.</li>
      </ul>
    </div>
  </body>
</html>

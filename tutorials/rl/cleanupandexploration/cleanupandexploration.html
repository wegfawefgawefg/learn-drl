<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Basic Page Needs
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8" />
  <title>Wegfawefgawefg's Tutorials</title>
  <meta name="description" content="" />
  <meta name="author" content="" />

  <!-- Mobile Specific Metas
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- FONT
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css" />

  <!-- CSS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css" />
  <link rel="stylesheet" href="/css/barebones.css" />

  <!-- Favicon
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon-16.png" />
  <!-- Code Highlighting
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/highlightjs/styles/atom-one-dark-reasonable.css" />
  <script src="/highlightjs/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
</head>

<body>
  <div class="grid-container full">
    <h1>Wegfawefgawefg's Tutorials</h1>
    <h2>Cleanup And Exploration</h2>
    <h3>Bringing The Code Up To Modern Standards</h3>
    <h4>Prerequisites</h4>
    <p class="u-align-left">This tutorial assumes you've completed the <a
        href="/index.html#foundations">"Foundations"</a> tutorials.
      It also uses code from those previous tutorials. The intention is to make the old code look more like
      common RL code in preperation for the coming <strong>upgrades</strong>. If you aren't new to reinforcement
      learning then i'm sure you can
      follow along. Less will be explained within the <a href="/index.html#upgrades">"Upgrades"</a> tutorials than prior
      tutorials. There might be a
      link to prior explanation from prior tutorials if I can remember to add the link.
    </p>
  </div>

  <div class="grid-container full">
    <h4>Getting Started</h4>
    <p class="u-align-left">
      The DQN implementations in prior tutorials worked, and functioned well as a demonstration of DRL.
      The intention was to minimize lines, and as such, minimize necessary explanation, and complication.
      However, that approach has flaws. The simplified code comes at a cost of both agent
      performance, and run speed. And that simplified code looks different from the common code seen on github.
      So comparing it to other people's agent code might not be as straightforward as it could be.
      Either way, it served it's purpose as a learning tool, and now that you are familiar with the function of
      each component of a drl agent it's time to modernize it a bit.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Code Review</h4>
    <p>This is what we are working with. There are some <code>HERE</code> comments marking what needs attention most.
    </p>
    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym       
import math
import numpy as np
import random

'''FINE: this class actually looks mostly fine as is.
Sure, the number of layers and layer dimensions aren't passed in as arguments 
to the constructor, but that's a lot of boilerplate I don't want to inflate the code with.
Yes, that is useful if you want to programmatically try different architectures, but 
it's not necessary generally.'''
class Network(torch.nn.Module):
    def __init__(self, alpha, inputShape, numActions):
        super().__init__()
        self.inputShape = inputShape
        self.numActions = numActions
        self.fc1Dims = 1024
        self.fc2Dims = 512

        self.fc1 = nn.Linear(*self.inputShape, self.fc1Dims)
        self.fc2 = nn.Linear(self.fc1Dims, self.fc2Dims)
        self.fc3 = nn.Linear(self.fc2Dims, numActions)

        self.optimizer = optim.Adam(self.parameters(), lr=alpha)
        self.loss = nn.MSELoss()

        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cpu")
        self.to(self.device)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)

    def chooseAction(self, observation):
        state = torch.tensor(observation).float().detach()
        state = state.to(self.network.device)
        state = state.unsqueeze(0)

        qValues = self.network(state)
        action = torch.argmax(qValues).item()

        '''HERE: this should probably be changed for a more sane exploration method.'''
        chanceOfAsparagus = random.randint(1, 10)
        if chanceOfAsparagus == 1:  #   10% chance
            action = random.randint(0, 1)

        return action

    def learn(self, memory, batchSize):
        '''HERE: theres no minimum memory size??'''
        if len(memory) < batchSize:
            return 

        self.network.optimizer.zero_grad()

        '''HERE: this entire block runs very slow compared to the common implementation'''
        randomMemories = random.choices(memory, k=batchSize)
        memories = np.stack(randomMemories)
        states, actions, rewards, states_, dones = memories.T
        states, actions, rewards, states_, dones = \
            np.stack(states), np.stack(actions), np.stack(rewards), np.stack(states_), np.stack(dones)
        
        states  =   torch.tensor( states    ).float().to(self.network.device)
        actions =   torch.tensor( actions   ).long().to(self.network.device)
        rewards =   torch.tensor( rewards   ).float().to(self.network.device)
        states_ =   torch.tensor( states_   ).float().to(self.network.device)
        dones   =   torch.tensor( dones     ).to(self.network.device)

        qValues = self.network(states)
        nextQValues = self.network(states_)

        batchIndecies = np.arange(batchSize, dtype=np.int64)

        '''HERE: the names of these variables don't match the common names, but 
        the functionality is all here, and the equation is fine. 
        Except it is missing one part, but that will be explained.'''
        nowValues = qValues[batchIndecies, actions]    #   interpret the past
        futureValues = torch.max(nextQValues, dim=1)[0]    #   interpret the future
        futureValues[dones] = 0.0   #   ignore future actions if there will 
                                    #   be no future actions anyways
        trueValuesOfNow = rewards + futureValues    #   same temporal difference
        loss = self.network.loss(trueValuesOfNow, nowValues)

        loss.backward()
        self.network.optimizer.step()

'''FINE: this "main agent loop" is actually really close to the common drl code.'''
if __name__ == '__main__':
    env = gym.make('CartPole-v1').unwrapped
    agent = Agent(lr=0.001, inputShape=(4,), numActions=2)
    BATCH_SIZE = 64
    '''HERE: the memory deserves an upgrade'''
    memory = []

    highScore = -math.inf
    episode = 0
    numSamples = 0
    while True:
        done = False
        state = env.reset()

        score, frame = 0, 1
        while not done:
            # env.render()

            action = agent.chooseAction(state)
            state_, reward, done, info = env.step(action)

            '''HERE: the memory will change so this will change a bit'''
            transition = [state, action, reward, state_, done]
            memory.append(transition)
            
            agent.learn(memory, BATCH_SIZE)
            state = state_

            numSamples += 1

            score += reward
            frame += 1

        highScore = max(highScore, score)

        print(( "total samples: {}, ep {}: high-score {:12.3f}, "
                "score {:12.3f}").format(
            numSamples, episode, highScore, score,frame))

        episode += 1</code></pre>
  </div>
  <div class="grid-container full u-align-left">
    <h3 class="u-align-center">Upgrades</h3>
    <p>The three areas in this code just asking for improvement are the experience replay buffer, the temporal
      difference
      code in the learn function, and the exploration strategy.
    </p>
    <h4 class="u-align-center">Exploration Strategy</h4>
    <p>At the moment the agent is picking a random action 10% of the time. This is to force
      the agent to try different actions. On a micro scale this has the effect of ensuring enough data is collected
      to get the neural network to replicate the environments reward function. But on the macro scale it pushes
      the agent into circumstances it wouldnt normally find itself in. I explain basic exploration in the
      <a href="/tutorials/rl/deepqlearning/deepqlearning.html">DQLearning Tutorial</a> and the
      <a href="/tutorials/rl/actorcritic/actorcritic.html">Actor Critic Tutorial</a>.
      The primary issue in the code as is, is the exploration rate. It doesn't change.
      As the qvalues become more refined, less exploration is necessary. Also the code has the word asparagus in it.
    </p>
    <pre><code class="language-python">class Agent(): # NEW AND IMPROVED
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)

        # exploration parameters
        self.epsilon = 0.1            # chance of random action
        self.epsilon_decay = 0.00005  # how much the chance shrinks each step
        self.epsilon_min = 0.001      # minimum for the chance, so you never fully stop exploring

    def chooseAction(self, observation):
        if np.random.random() < self.epsilon: # generate a num between 0.0 and 1.0 to "roll"
            action = random.randint(0, 1)
        else: # dont bother doing all that torch stuff if you're just gonna choose a random
            state = torch.tensor(observation).float().detach()
            state = state.to(self.network.device)
            state = state.unsqueeze(0)

            qValues = self.network(state)
            action = torch.argmax(qValues).item()
        return action</code></pre>
    <p>The new improved code uses the <strong>epsilon-greedy</strong> exploration strategy. It works just like the old
      exploration strategy:</p>
    <pre><code class="language-python">chanceOfAsparagus = random.randint(1, 10)
if chanceOfAsparagus == 1:  #   10% chance
    action = random.randint(0, 1)</code></pre>
    <p>
      Except instead of a fixed 10% chance, the chance starts high and shrinks until it hits a minimum.
      Why is it called epsilon? More math history or something. It is generally denoted by the <code>ϵ</code> character.
      I think it appeared around or before 1998, maybe in the Sutton and Barto book on reinforcement learning.
      Anyways, epsilon is a number between 0 and 1, that should
      start high, and slowly become smaller. Generate a random number between zero and one, and then compare it to
      your exploration threshold, <strong>epsilon</strong>. If epsilon is 0.9, then 90% of the time the random number
      will be smaller than epsilon, so 90% of the time the action will be random.

      <strong>Epsilon-greedy</strong> is the most common exploration strategy I've seen.
      You will see it all over reinforcement learning.
    </p>
    <h5>Common Concerns</h5>
    <p>Dont forget to actually shrink epsilon, and cap it at the minimum value.
      You can do it periodically but I most commonly see the decay done once per learn step at the bottom of
      the learn function.
    </p>
    <pre><code class="language-python">def learn(self, memory, batchSize):
      ... # bla bla bla the rest of the learn function above
      trueValuesOfNow = rewards + futureValues    #   same temporal difference
      loss = self.network.loss(trueValuesOfNow, nowValues)
      
      loss.backward()
      self.network.optimizer.step()
      
      '''SHRINK EPSILON HERE'''
      self.epsilon -= self.epsilon_decay  # shrink
      if self.epsilon < self.epsilon_min: # clamp
      self.epsilon = self.epsilon_min</code></pre>
    <p>
      <div class="grid-container halves u-align-left">
        <div class="left">
          <h5>Greater Than Less Than</h5>
          <p>Make sure to get the direction of the <code>&lt</code> vs <code>&gt</code> correct when comparing to
            epsilon to pick a random action. If you get it backwards, your exploration chance is inverted.</p>
          <pre><code class="language-python"># epsilon = 0.1

# this is 10% chance
if np.random.random() < self.epsilon:   
    action = random.randint(0, 1)

# this is 90% chance
if np.random.random() > self.epsilon:   
    action = random.randint(0, 1)</code></pre>
          <p>And worse, if it is backwards, your exploration chance will actually increase instead of decrease when you
            shrink epsilon. As you can imagine, this results in terrible scores.
          </p>
          <h5>When In Doubt, Print</h5>
          <p>If the agent is being dumb, print out epsilon at each step or episode to make sure
            it is doing what you want.
          </p>
        </div>
        <div class="right">
          <h5>How To Set Settings</h5>
          <p>That epsilon_decay number I picked seems kinda random doesn't it?</p>
          <pre><code class="language-python">self.epsilon = 0.1    
self.epsilon_decay = 0.00005  
self.epsilon_min = 0.001</code></pre>
          <p>If you set these wrong the agent won't learn at all, and it becomes an exercising in diagnosing bugs. You
            might
            think it's an issue with any other part of your code, which could send you on an hours/days bug hunt.
            Luckily, these new hyperparameters can be made to be equivalent to the old code.
          </p>
          <pre><code class="language-python">self.epsilon = 0.1    
self.epsilon_decay = 0.0
self.epsilon_min = 0.1</code></pre>
          <p>If epsilon is 0.1, and the decay rate is 0.0, then it is a constant flat 10% chance of random action.
            This should yield exactly the same results as the old code. (It does. I tested it.)
            So you can start from this, and slowly shrink the epsilon from there. <code>self.epsilon_decay =
              0.00000001</code>
            One of the benefits of this standard epsilon-greedy implementation is you can now use other peoples
            settings.
            Yay.
          </p>
        </div>
      </div>
      <h5>The Future of Exploration</h5>
      Epsilon-greedy is not perfect.
      Notice epsilon can only decrease, and never increase. This could be considered a feature,
      but also a flaw.
      It makes assumptions about how the best strategy is developed, and about how unchanging the environment is.
      When a human finds that it is not getting good results, it actively tries new strategies. Animals use adaptive
      exploration rates.
      Maybe the agent's exploration rate should increase if reward is unexpectedly low.
      Maybe it should change if the reward is staying the same for a while.
      Perhaps it should decrease based on queues from the environment, and should be a function learned across
      environments
      with a neural network.
      It's a whole other upgrade just waiting to be discovered.<br>
      Another issue with this exploration strategy is that it explores by randomizing action,
      as opposed to randomizing the goal. Humans practice tasks often by learning similar modified versions of the task.
      Even if somebody doesn't know what optimal chess looks like, they still know they will get better at chess by
      playing alternate versions of chess, and chess minigames. Sometimes this alternate goal
      can be drastically different. For example, you can get better at shooting an arrow at a target, by practicing
      missing the target on purpose by a certain amount. This goal exploration effect could be achieved by
      micromanaging which environments the agent plays, or it could be built into the agent. Where the agent will
      sometimes try different goals, and ignore the environmental reward.
      Another upgrade worth investigating.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Numpy Experience Replay</h4>
    <p>Maybe you noticed before, but the old code gets slower over time.
      This is partially due to the agent getting better at the game. As the agent gets better, the episodes require
      more steps to end. However, you probably noticed if you let the game go for a few thousand episodes, the episodes
      start to get really slow, even though the scores are roughly the same. (For the record, a few thousand 
      episodes is not that many. DRL papers often consider agents trained on millions of samples.)
      What is the cause of this slowdown? 
      Nothing in our agent seems to require more processing at a later episode than an early episode. 
      It should be a constant amount of processing per learn step... except the code involving the experience replay buffer. 
      Thats not too much of a problem for "read", but the moment you need to "write" back to the buffer inside 
      your learn step, runtime performance suffers too much. It starts to get unbearably slow.<br>
      Additionally, your options for batch size are limited by the sampling speed of your replay buffer.
      As the replay buffer gets longer, sampling from it becomes slower. (relative to numpy arrays atleast)
      We mostly avoided that effect by being careful up to this point. But it's better to have a high power 
      tool to play with. 
      A lot of the agent upgrades involve sampling data from the memory, 
      computing something, and maybe even storing. Doing this with python arrays starts to become a performance problem, 
      especially once there are some python for loops in there. ew :^) 
      So think of this as an infrastructural investment. The numpy indexing tricks end up being incredibly convenient, and 
      besides, it's the most common way to do memories anyways.
    </p>
    <p>So, this:</p>
    <pre><code class="language-python">memory = []</code></pre>
    <p>becomes:</p>
    <pre><code class="language-python">class ReplayBuffer(): '''NEW CODE'''
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0

        self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64)
        self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
        self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool)

    def storeMemory(self, state, action, reward, nextState, done):
        memIndex = self.memCount % self.memSize 
        
        self.stateMemory[memIndex]      = state
        self.actionMemory[memIndex]     = action
        self.rewardMemory[memIndex]     = reward
        self.nextStateMemory[memIndex]  = nextState
        self.doneMemory[memIndex]       = done

        self.memCount += 1

    def sample(self, sampleSize):
        memMax = min(self.memCount, self.memSize)
        batchIndecies = np.random.choice(memMax, sampleSize, replace=False)

        states      = self.stateMemory[batchIndecies]
        actions     = self.actionMemory[batchIndecies]
        rewards     = self.rewardMemory[batchIndecies]
        nextStates  = self.nextStateMemory[batchIndecies]
        dones       = self.doneMemory[batchIndecies]

        return states, actions, rewards, nextStates, dones</code></pre>
    <p>Notice how much code this adds. There's a reason i don't encourage
      people to start with this.</p>
    <h5>Struct Of Arrays</h5>
    <p>Each piece of the transition is stored in its own array. You premake and the arrays to a certain size when 
      you create the memory.
    </p>
    <pre><code class="language-python">self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64)
self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool)</code></pre>
    <p>Notice the names of these arrays correspond to the same SARS you are familiar with.</p>
    <div class="grid-container halves u-align-left">
      <div>
        <h5>Class Inputs</h5>
        <pre><code class="language-python">class ReplayBuffer(): '''NEW CODE'''
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0</code></pre>
        <p>
          The class takes in the max size of the buffer, and the shape of the states.
          Something important to note here is that <code>stateShape</code> needs to be a tuple,
          not a single number. <br>In python <code>(4,)</code> and <code>(4)</code> are
          not the same thing. It's a noob trap.<br><br>
          Ex: cartpole has a state shape of 4 numbers, so the stateShape
          should be <code>(4,)</code>. If you just pass in <code>stateShape=4</code> it wont work.
          That's because there is some <strong>tuple unpacking</strong> going on in the arrays.
          (See the <code>*stateShape</code> in the array allocation.)<br>
        </p>
        <h5>Sampling</h5>
        <p>Getting the transitions out isn't so difficult, and most importantly, it
          takes the same amount of time, regardless of how big the memory is.
        </p>
        <pre><code class="language-python">batchIndecies = np.random.choice(
    memMax, sampleSize, replace=False)

states      = self.stateMemory[batchIndecies]
actions     = self.actionMemory[batchIndecies]
...</code></pre>
        <p>First pick some random indices, and then just use the numpy indexing magic
          to get all the right elements out at once.
        </p>
      </div>
      <div>
        <h5>Indexing Complications</h5>
        <p>You may have noticed a few weird lines.</p>
        <pre><code class="language-python">self.memCount = 0
# and 
memIndex = self.memCount % self.memSize
# and
memMax = min(self.memCount, self.memSize)</code></pre>
        <p>The way this replay buffer works is the arrays are given a length before hand.
          So we have to keep track of how many memories were currently stored to know where to
          put the new transitions. <code>self.memCount += 1</code></p>
        <p>What happens if you store memories after the buffer is full?</p>
        <pre><code class="language-python"># the index rolls over back to the begining.
memIndex = self.memCount % self.memSize 

#  this overwrites the oldest memory
self.stateMemory[memIndex]      = state
self.actionMemory[memIndex]     = action
self.rewardMemory[memIndex]     = reward
self.nextStateMemory[memIndex]  = nextState
self.doneMemory[memIndex]       = done</code></pre>
        <p>Before the memory has been filled all the way, you have to ignore indices that haven't been
          assigned data to yet. They will just hold garbage.
        </p>
        <pre><code class="language-python">#  that should explain this line in sample()
memMax = min(self.memCount, self.memSize)</code></pre>
      </div>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Now To Use It</h4>
    <p>To use the new replay buffer we have to go put it in the main loop, and then plop it into the 
      learn function where the "extract-and-stack" code was.
    </p>
  </div>
  
  <div class="grid-container full u-align-left">
    <p>
      The predictive function behind the <strong>action picker</strong> or the
      <strong>state value estimator</strong> can be whatever you want (ex:
      linear regression, nearest neighbor, genetic algorithm, random forest).
      However i will be using a single neural network for both of them.
    </p>
    <pre><code class="language-python">class ActorCriticNetwork(torch.nn.Module): #   ~here is my handle~
    def __init__(self, lr, inputDims, numActions):
        super().__init__()
        # fc means "fully connected layer", 
        self.fc1Dims = 1024  #   size of hidden layer 1
        self.fc2Dims = 512   #   size of hidden layer 2

        #   backbone network
        self.fc1 = nn.Linear(*inputDims, self.fc1Dims)     #   hidden layer 1
        self.fc2 = nn.Linear(self.fc1Dims, self.fc2Dims)   #   hidden layer 2

        #   tail networks       ~here is my spout~
        <strong>self.actor = nn.Linear(self.fc2Dims, self.numActions)   #   here is the actor</strong>
        <strong>self.critic = nn.Linear(self.fc2Dims, 1)                #   here is the critic</strong>
        
        #   pytorch stuff
        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, observation): #   ~tip me over and pour me out~
        state = torch.tensor(observation).to(self.device)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        <strong>policy = self.actor(x)  #   actor outputs one number for each action, a vector</strong>
        <strong>value = self.critic(x)  #   critic just puts out the value, one number</strong>
        return policy, value</code></pre>
  </div>
  <div class="grid-container thirds u-align-left">
    <div>
      <h5>Network Class Inputs</h5>
      <p>
        The network class takes in the learning rate (often called alpha), game frame shape
        (the inputs for the actor-critic network), and the number of actions (which happens to be the output
        of the actor network).
      </p>
    </div>
    <div>
      <h5>Tail Networks</h5>
      <p>
        A convenient way to do an actor-critic network is by making them share
        a backbone network. And it makes sense, the same features that are
        useful for determining state value are probably also useful for
        determining which action to pick next.
      </p>
    </div>
    <div>
      <h5>Policy</h5>
      <p>
        The values used to pick actions are known as the
        <strong>policy.</strong>
        Why that word? Math history or something. Not a terrible word for it
        thought.
        <br />
        "It is my policy to do THIS under THESE specific circumstances." The
        standard symbol for a policy is 𝜋. (pi, like 3.14pi) <br />
        In reinforcement learning papers anytime you see 𝜋 it means
        <strong>policy</strong>.
      </p>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Picking an Action</h4>
    <p>
      For vanilla Deep Q learning it's as easy as picking the highest actor
      action output. :)
    </p>
    <pre><code class="language-python">def chooseAction(state):
    policy, _ = actorCriticNetwork(state)
    <strong>action = torch.argmax(policy).item()    #   pick here</strong>
    return action</code></pre>
    <p>
      But this isnt Vanilla Deep Q Learning. This is Actor-Critic Method.
      >:()<br />
      The <strong>actor</strong> outputs probabilities of actions instead of
      actual action values.<br />
      So the <strong>Eat:</strong> 0.8 from earlier, yeah thats like an 80%
      chance of eating the burger.<br />
      If this was DQ Learning a value of 0.8 being the highest would mean 100%
      chance of eating the poison burger. :(
    </p>
    <p>Lets choose an action the Actor Critic way.<br>
      We start by passing in the state from the environment into the critic.
      I don't know why this convention, but people often call the state: "observation".
      Maybe because it's what the agent "observes"?</p>
    <pre><code class="language-python">def chooseAction(observation):
    policy, _ = actorCritic.forward(observation)
    policy = F.softmax(policy, dim=0)
    actionProbs = torch.distributions.Categorical(policy)
    action = actionProbs.sample()
    self.logProbs = actionProbs.log_prob(action)    #    saving this value for later
    return action.item()</code></pre>
  </div>
  <div class="grid-container halves u-align-left">
    <div class="left">
      <h5>Softmax</h5>
      <p>
        We have to softmax the output action probabilities from the networks
        output <strong>policy</strong>.<br />
        Why are the outputs not good enough as is? Well, its just because the
        policy is supposed to be probabilities of each action being taken.
        <br />
        Probabilities need to add up to 1.<br /><br />
        Consider the following policy:<br />
        <strong>Eat:</strong> 0.8 <br />
        <strong>Dont Eat:</strong> 0.3 <br />
        <strong>Be Suspicious:</strong> 0.001 <br /><br />
        Thats 80% + 30% + 0.001%. <br />
        110.001% with the actions combined. <br />
        "110% chance of rain today."
        Doesn't make any sense right? <br />
        Softmax takes in a list of numbers and makes them add up to 1. Which
        is exactly what we want.<br />
      </p>
      <pre><code class="language-python">#    softmax example
a = torch.tensor([0.8, 0.3, 0.001])
b = F.softmax(a, dim=0)
b is now tensor([0.4863, 0.2950, 0.2187])</code></pre>
      <p>
        You may have noticed now <strong>Eat</strong> has changed size
        relative to <strong>Dont Eat</strong>. 0.8 / 0.3 is not the same as
        0.4863 / 0.2950<br />
        Softmax mangles the relative probability scales just a little bit.
        But, the network figures it out after a while. More importantly,
        thanks to softmax no matter what weird shit the actor network outputs
        we have sane action probability ranges.
      </p>
    </div>
    <div class="right">
      <div>
        <h5>Categorical Distribution</h5>
        <p>
          You put 80 red marbles, 30 blue marbles, and a glass shard from a
          green marble in a bag. We have 3 distinct
          <strong>categories</strong> of marbles. One might even say its a
          categorical distribution of marbles.<br />
          You pick one marble from the bag. One might even say you
          <strong>sample</strong> the bag.<br />
          In this case sample() returns an index because its from a
          categorical distribution. But from other distributions it might
          return a floating point value. Another common distribution used for
          picking actions in reinforcement learning is a normal distribution.
        </p>
      </div>
      <div>
        <h5>Log Probability</h5>
        <p>
          This is the probability of a specific action, but pushed through the
          log function.<br />
          Why do this? Well like everything in life there are more than one
          explanation, and some of them are more complicated than others.
          Basically, you'll understand when you're older.<br />
          You'll also understand it right now:
        </p>
        <ul>
          <li>
            One explanation involves some math nonsense i think someone made
            up to sound cool.
            <a href="https://www.quora.com/What-is-log-probability-in-policy-gradient-reinforcement-learning">here</a>
          </li>
          <li>
            One explanation involves the log probability having more stable
            scaling than the raw probability.
            <a
              href="https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability">here</a>
          </li>
        </ul>
        <p>
          The important thing to take from this though is that you are gonna
          need the probability of the chosen action for teaching the network.
          And smoothing and shrinking the distribution out gives the network
          an easier time.<br />
          Specifically, youre gonna multiply the probabilities of chosen
          actions by the critic state values. Your natural intuition for why
          this works is probably pretty reasonable. We'll get to the
          explanation in a sec.
        </p>
      </div>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h5>Determinism</h5>
    <p>Why is our actor putting out chances of actions? Seems kind of stupid doesn't it?
      Imagine that every millisecond you are holding a knife you have a 10% chance of letting go of it.
      That's dumb right? Why not just pick the highest action instead of sampling a distribution?
      Well most of the time the next millisecond you will decide to hold the knife tightly again.
      So statistically it works out. You will never completely drop the knife.
      The Actor-Critic algorithm statistically makes good decisions, but it might make a different
      decision given the same exact scenario a second time. It is <strong>non deterministic</strong>.<br>
      In the real world with fuzzy inputs like muscle flexing and rotation angles, picking the wrong
      action for small fraction of a second is not that big of an issue.
      For a game where a single wrong button makes you lose, it kind of sucks.<br>
      Some reinforcement learning algorithms ARE deterministic. But that has its own downsides.
      Non-deterministic algorithms naturally explore different actions by accident. They try new things sometimes.
      Deterministic algorithms can get stuck, repeating the same action every time, until it gets punished into
      oblivion.
      Sometimes deterministic algorithms are so stubborn they will pick the same action until it has been punished so
      much
      <strong>that it will never choose that action ever again. ever.</strong> You will likely even see this happen if
      you keep
      playing with AI Gym and Deep Q Learning.
      Deterministic algorithms often need some noise injected into the decision making process to help them be less
      stubborn.
      Luckily for us, Actor-Critic doesn't need that because it is <strong>non-deterministic</strong>.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 id="improving-our-choices-in-life">Improving Our Choices In Life</h4>
    <p>
      The actor network and the critic network both have unique errors.<br />
      Each network is punished by their own respective error, and their
      estimation of their respective responsibility improves. We get better
      actions from the <strong>actor</strong>, and better value estimates from
      the <strong>critic</strong>.
    </p>
    <pre><code class="language-python">def learn(state, reward, nextState, done): <a>following the meta</a>
    actorCritic.optimizer.zero_grad()  #   pytorch stuff: resets all the tensor derivatives

    #   fetch values from the critic network
    _, criticValue = actorCritic.forward(state)            #   the value of now
    _, nextCriticValue = actorCritic.forward(nextState)    #   the value of the future

    #   the temporal difference zone
    #   #   the true value of now = now + future
    valueOfNow = reward + <strong>gamma</strong> * nextCriticValue * (1 - int(done))    
    temporalDifference = valueOfNow - criticValue    <a>oh how wrong we were</a>

    #   compute the error for our actor and critic networks
    actorLoss = -self.logProbs * temporalDifference  #  probability of chosen action times how wrong we were
    criticLoss = temporalDifference**2   #   we dont care about which direction (the sign), 
                                         #   #   just wanna minimize how wrong we were in total

    (actorLoss + criticLoss).backward()
    actorCritic.optimizer.step()</code></pre>
  </div>






  <div class="grid-container halves u-align-left">
    <div class="left">
      <div>
        <h5>Learn Function Inputs</h5>
        <ul>
          <li>
            <strong>State</strong> is our game screenshot, or robot arm
            position, or self driving car position/velocity. It's the same
            thing that goes into the actor and critic.
          </li>
          <li>
            <strong>Reward</strong> is the ground truth
            <strong>value</strong> that our critic is supposed to learn. It is
            generally returned by an environment. In AI Gym it is explicity
            the reward, but for your own environment it could be how much
            money your trading algorithm made, or how well balanced your robot
            is, or how many non poisonous burgers you ate. The
            <strong>reward</strong> is a hedonistic measure of success.
            Success right now. <br />
            It's magnitude is kind of arbitrary. Could be 1.0, could be 100.0.
            <br />
            It doesn't matter what scale it is, just that it is consistent.
          </li>
          <li>
            The <strong>nextState</strong> is just like
            <strong>state</strong>, except its the next one.<br />
            It is the future, one time step forward. That means in order to
            run this we already need to know the
            <strong>nextState</strong>.<br />
            <strong>You don't learn from the present. You learn from comparing the
              present to the past.</strong>
          </li>
          <li>
            <strong>Done</strong> is just whether the
            <strong>nextState</strong> was the last one. (true or false, 1 or
            0)<br />
            If the game was game over on <strong>state</strong> then
            <strong>nextState</strong> isn't valid.<br />
            You don't care how much money you made the day after you died.
            Because you are dead. <a>see (1 - int(done))</a><br />
            The vast majority of our <strong>nextState</strong>s will not be
            the end of the game/episode/trial, so most of the time it doesn't
            even get used. It will just be false/0.
          </li>
        </ul>
      </div>
    </div>
    <div class="right">
      <div>
        <h5>Temporal Difference</h5>
        <p>
          This is really important for reinforcement learning. You will see
          variations of it all over the place.<br />
          <strong>Temporal Difference</strong> is a way of valuing the
          present.<br />
          The true value of now includes the potential value of all future
          states.<br />
          We can compute that in a literal way. Just add the value of now to
          the value of the future.<br />
          We discount the future a little bit by multiplying it by
          <strong>gamma</strong> which is normally 0.99 ish.
        </p>
        <pre><code class="language-python">actualValueOfNow = reward + gamma * valueOfTheFuture
temporalDifference = actualValueOfNow - criticsGuess</code></pre>
        <p>
          A key difference though, notice how in our learn function we dont
          pass in the next rewards. That's because instead of using the ground
          truth future value we just let the critic guess.
        </p>
        <pre><code class="language-python">_, nextCriticValue = actorCritic.forward(nextState)
valueOfNow = reward + gamma * nextCriticValue</code></pre>
      </div>
      <div>
        <h5>Gamma</h5>
        <p>
          <strong>Gamma</strong> is known as the discount factor. In
          reinforcement learning math you will see it as γ.<br />
          We have to discount the future a little bit. Future rewards are not
          as valuable as rewards now. I will explain more after you have some
          more context for these types of algorithms.
        </p>
      </div>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h5>One Quick Trick</h5>
    <pre><code class="language-python">(actorLoss + criticLoss).backward()</code></pre>
    <p>This might seem weird if you know pytorch. Why not just call backward on actorLoss, then on criticLoss?
      Well, because they share a backbone network, you can't do that. Pytorch will complain about "unreachable gradient"
      stuff when you call step().
      And, actually, I think it computes the wrong gradient. Order will matter for the shared backbone network. Luckily
      for us the derivative of addition is 1.
      So adding the losses togethor just multiplies the gradient by 1. Multiplying things by 1 doesn't change them.
      :^)<br>
      If you want to split your network in all sorts of crazy ways this works for adding as many as you want togethor.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Formal Representation</h4>
    <p>
      If you want to see the way mathemagicians formalize temporal difference,
      <a href="https://en.wikipedia.org/wiki/Temporal_difference_learning#Mathematical_formulation">here you go</a>. I
      urge you to give the equations a look. I bet you can kinda get
      them. You're gonna need to wet your whistle on these a little bit if you
      want to go implement algorithms from professional reinforcement learning
      papers.
    </p>
    <h5>A Guide For The Overly Formal Notation:</h5>
    <div class="grid-container thirds u-align-left">
      <div>
        <h5>Policy and Value</h5>
        <ul>
          <li>𝜋 means policy</li>
          <li>s means state</li>
          <li>V means value</li>
        </ul>
        <p>
          So, V<sup>𝜋</sup>(s) means the value of a state according to our
          policy.
        </p>
      </div>
      <div>
        <h5>Time</h5>
        <ul>
          <li>s<sub>1</sub> just means the state after s<sub>0</sub>.</li>
          <li>s<sub>t</sub> just means the state at any time.</li>
        </ul>
        <p>
          So, logically s<sub>t+1</sub> is the state after s<sub>t</sub>.
          <br />
          Little t is used for time.
        </p>
      </div>
      <div>
        <h5>Big Sigma</h5>
        <p>Σ is just a for loop where you add the result to a total.</p>
        <p>So Σ<sup>10</sup><sub>t=0</sub> t is this:</p>
        <pre><code class="language-python">total = 0
for i in range(0, 10 + 1):
    total += i</code></pre>
        <p>
          The <strong>sum</strong> of the inside stuff where t goes from 0 to
          10.
        </p>
      </div>
    </div>
    <p>
      For the most part reinforcement learning math boils down to formalizing
      these questions:<br />
      What's the value of now? What's the value of the future? What's the
      value of an action? What's the total value of all the actions i took so
      far? What's the total value of all the best possible actions?
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Loss</h4>
    <p>
      So just like most neural network stuff we want the loss to go to zero.
      But, in reinforcement learning its a little more subtle than that. The
      <strong>actor</strong> and <strong>critic</strong> both have different
      losses. And they function fairly differently as well.<br />
      This is where we focus on these lines specifically:
    </p>
    <pre><code class="language-python">actorLoss = -self.logProbs * temporalDifference
criticLoss = temporalDifference**2</code></pre>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Critic Loss</h5>
        <p>
          For the <strong>critic</strong> the loss is just the difference
          between what it thought the reward was now, and what the reward
          actually was. It doesn't matter what direction the error is, + or -.
          Just want to minimize it. Hence squaring it to remove the sign.
        </p>
      </div>
      <div class="right">
        <h5>Actor Loss</h5>
        <p>
          For the <strong>actor</strong> punishment, we take the probability
          of our chosen action, and multiply it into how wrong our state value
          estimation was.<br />
          Why? There are two reasons.
        </p>
      </div>
    </div>
  </div>
  <div class="grid-container full">
    <h5>Actor Loss Subtleties</h5>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Make It Zero</h5>
        <p>
          Remember our <strong>action probabilities</strong> are between 0 and
          1.<br />
          Remember we pass those probabilities into log() when we compute the
          <strong>log-probabilities</strong>?<br />
          The graph of log(x) is 0 at x = 1. (go look at the graph now).<br />
          If our action choice was perfect, the probability of that action
          should be 100% or 1.0 (That way we pick that action every time).<br />
          When you put the output from the actor of 1.0 into log(x) you get 0.
          So the <strong>actor</strong>'s goal is to pick an action such that
          log(actorOutput) == 0, a perfect action.<br />
          That means the error should be designed such that if we did a
          perfect action, error should be zero.<br />
          How do you know if the action was perfect? Well the action was
          perfect if the <strong>critic</strong> output the correct value for
          the state. The action is as wrong as the
          <strong>critic</strong> times the confidence of the
          <strong>actor</strong> choice.<br />
          <strong>This means our actions can only be as good as our value
            estimation.</strong>
          This can't be understated. It has important implications for the
          instability of
          <strong>value</strong> based learning. I will discuss it more
          another time.
        </p>
      </div>
      <div class="right">
        <h5>The Four Cases</h5>
        <p>
          Why the multiplication, and why the negative on the actor loss?
          Consider the following 4 cases:
        </p>
        <ul>
          <li>
            The critic slightly overestimated the reward.<br />
            <pre><code class="language-python">(reward + nextReward) < criticValue  #   so...
smallNegativeValue = (reward + nextReward) - criticValue</code></pre>
            If the critic overestimates that means the action wasn't as good
            as we thought it was. So we should shrink the probability. We have
            a small negative temporal difference value, so to shrink the
            probability we make that positive, and then multiply the small
            number by our probability.
          </li>
          <li>
            The critic slightly underestimates the reward.<br />
            Our action wasn't confident enough. Our TD was positive. <br />
            <pre><code class="language-python">(reward + nextReward) > criticValue  #   so...
smallPositiveValue = (reward + nextReward) - criticValue</code></pre>
            We need to increase that action's probability. It should move in the opposite
            direction of the last example. That's why the sign is reversed. And the
            number is small because it should be a small increase.
          </li>
        </ul>
        <p>
          The remaining two cases are the same, just with different magnitudes
          of adjustment.
        </p>
        <ul>
          <li>The critic largely overestimates the reward.</li>
          <li>The critic largely understimates the reward.</li>
        </ul>
        <p>
          The point is that the action probability should be adjusted
          proportionally to how wrong or right the
          <strong>critic</strong> was, and in the correct direction. Negative
          for shrinking, and positive for growing.<br>
          I want to give more specific examples, but in order to make the example simple enough
          to make sense it becomes a pretty impractical. Maybe someone has a good one somewhere.
          In this particular case if you flip the sign your lander will get worse at landing over time
          instead of better. Also if you multiply in a huge or tiny constant into the TD to change the magnitude, you
          can see
          the lander overcompensate and undercompensate when it tries to balance. It will eventually figure that out
          though. The network
          weights just adjust to be smaller or bigger.
        </p>
      </div>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Agent</h4>
    <p>
      A reinforcement learning <strong>agent</strong> is what learns about the
      environment and chooses the actions.<br />
      There are many different types of <strong>agent</strong>s, but this one
      contains an <strong>actor</strong> and a <strong>critic</strong>.<br />
      An <strong>agent</strong> class doesn't need to explicitly exist but
      doing it this way is fairly neat, and makes it easy to swap in different
      agents in our main loop when we want to try out different ones.<br />
      To make the agent we just put the learn and choose action functions
      togethor with the actor critic network from earlier.<br />
      There really isn't anything new here. Just notice it's where we save our
      <strong>logProb</strong> when we choose an action.
    </p>
    <pre><code class="language-python">class ActorCriticAgent():
    def __init__(self, lr, inputDims, numActions):
        self.gamma = 0.99    #   a common gamma value
        self.actorCritic = ActorCriticNetwork(lr, inputDims, numActions)
        self.logProbs = None    #   log of the probability of the last action the agent chose

    def chooseAction(self, observation):
        policy, _ = self.actorCritic.forward(observation)
        policy = F.softmax(policy, dim=0)
        actionProbs = torch.distributions.Categorical(policy)
        action = actionProbs.sample()
        self.logProbs = actionProbs.log_prob(action)    #   save it here
        return action.item()

    def learn(self, state, reward, nextState, done):
        self.actorCritic.optimizer.zero_grad()

        _, criticValue = self.actorCritic.forward(state)
        _, nextCriticValue = self.actorCritic.forward(nextState)

        reward = torch.tensor(reward, dtype=torch.float).to(self.actorCritic.device)
        td = reward + self.gamma * nextCriticValue * (1 - int(done)) - criticValue

        actorLoss = -self.logProbs * td
        criticLoss = td**2

        (actorLoss + criticLoss).backward()
        self.actorCritic.optimizer.step()</code></pre>
  </div>
  <div class="grid-container full u-align-left">
    <h4>The Main Loop</h4>
    <p>
      All the hard parts are done now. The only thing left is to make our
      <strong>agent</strong>, put it into an environment, and trap it in an
      infinite loop so it can self improve until it becomes skynet. You can
      create your own environment or use input from the real world, but for
      demonstration let's use AI Gym. A place where AI can watch their macros
      and get sick gains. (or fail horribly. Some of the provided AI Gym
      environments are fairly difficult, and require much more complicated
      algorithms than this one to solve.)<br />
      AI Gym environments are nice little simulated worlds, that happen to
      return rewards and states just like our <strong>actor</strong> and
      <strong>critic</strong> needs.<br />
      What a coencidence. :^) Anyways, put our <strong>agent</strong> into an
      AI Gym environment and let it run for 20 minutes to 10 years.<br />
    </p>
    <pre><code class="language-python">agent = ActorCriticAgent(lr=0.00001, inputDims=(8,), numActions=4) <a>we wrote this earlier</a>
env = gym.make("LunarLander-v2")

highScore = -math.inf
recordTimeSteps = math.inf
while True:                     <strong>#   keep starting new episodes forever</strong>
    observation = env.reset()   <strong>#   observation is just a commonly used term for the environment state</strong>
    score, frame, done = 0, 1, False
    while not done:             <strong>#   keep going until the episode is done</strong>
        env.render()            <strong>#   draw it on your screen so you can watch</strong>
        action = agent.chooseAction(observation)    <a>we wrote this too</a>
        nextObservation, reward, done, info = env.step(action)  <strong>#   make the environment go one time step</strong>
        agent.learn(observation, reward, nextObservation, done) <a>and this</a>
        observation = nextObservation
        score += reward
        frame += 1

    recordTimeSteps = min(recordTimeSteps, frame)
    highScore = max(highScore, score)
    print(( "ep {}: high-score {:12.3f}, shortest-time {:d}, "
            "score {:12.3f}, last-epidode-time {:4d}").format(
        episode, highScore, recordTimeSteps, score, frame))</code></pre>
  </div>
  <div class="grid-container full">
    <h5>Agent Settings</h5>
    <div class="grid-container thirds u-align-left">
      <div>
        <h5>Input Dims</h5>
        <pre><code>inputDims=(8,)</code></pre>
        <p>This is 8 for the lunar lander environment because the state is 8 floats representing angle, and
          distance to the target and whatnot. As long as your network is taking them all in it doesn't really matter
          what they are.
          Although, normalizing them between -1.0 and 1.0 can help a lot of the time. It's a tuple, not a number,
          so that if the input was pixels it could be a tuple of width and height of the incoming game pixels.
        </p>
        <h5>Num Actions</h5>
        <pre><code>numActions=4, </code></pre>
        <p>This is the number of actions the lunar lander environment accepts. We only need one probability for each
          action the agent can take. Make sure if you switch to other environments to change your number of network
          outputs.
          You can get creative with this though. Wanna try normal distributions instead? You'll need 2 outputs per
          action.
        </p>
      </div>
      <div>
        <h5>Layer Sizes</h5>
        <pre><code>layer1Size=1024, layer2Size=512</code></pre>
        <p>I didn't pass in the network layer sizes for simplicity of example, but you can write your class so that you
          can pass them into the constructor for "rapid" experimentation if you want. There wasn't too much
          experimentation
          on my part to get these particular sizes. I found someone using these layer sizes online somewhere.
          I tried smaller ones but the agent never got good performance. <br>
          You could write a program to test varying neural network shapes and sizes and graph them out to find optimal
          agent settings for this. A lot of machine learning papers do just that. It can be very time consuming though.
          For a big agent it can be impractical, as it requires you to train your agent maybe 20 or more times to find
          good settings. <br>
          Either way I might make a tutorial for it at some point. Just remember, layers too small and it won't learn,
          or
          wont have a brain big enough to learn complicated behaviour. Layers too big and it runs slow.
          One of these is much worse than the other.
        </p>
      </div>
      <div>
        <h5>Tiny Alpha / Learning Rate</h5>
        <pre><code>lr=0.00001, </code></pre>
        <p>You might notice the learning rate is rather small if you are familiar with other machine learning stuff.
          Often I don't see reinforcement learning algorithms using the classic ML 10e-3 and 10e-4. Instead I see really
          really tiny alphas.
          Some algorithms can handle more normal learning rates, but this one can not. It turns out
          <strong>value</strong> based algorithms (that are concerned with assigning <strong>value</strong> to states),
          such as our critic, are rather unstable. You can try making it higher. I tried.<br>
          The cool thing is that it learns differently with different learning rates. With a high alpha it will learn
          how to balance the lander
          almost instantly, because that reward is so prominent and obvious. The uncool part is that it will stop at
          that and never learn how to land.<br>
          With a tiny learning rate it takes a pathetically long time to correctly balance the lander,
          overcompensating and undercompensating the thrusters. When it finally can balance it slowly inches it's hover
          closer and closer to the target over tens of episodes.
          But it will eventually successfully land, and consistently. It just never gets that with the high learning
          rate. Play with it.
        </p>
      </div>
    </div>
  </div>
  <div class="grid-container full">
    <h3>Full Code</h3>
    <div class="grid-container full u-align-left">
      <p>You did it, it's done. Heres the full code with all the imports added and sassy comments removed.
        Copy it into an editor and print the outputs of functions you dont understand.<br>
        If you don't have cuda go uncomment out the <code>torch.device("cpu")</code> line in the ActorCriticNetwork
        class to enable cpu mode.
        It will run much more slowly though so you might want to make the layer sizes smaller. Ex: layer1Size 128,
        layer2Size 128
      </p>
      <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import numpy as np

class ActorCriticNetwork(torch.nn.Module):
    def __init__(self, lr, inputDims, numActions, fc1Dims=1024, fc2Dims=512):
        super().__init__()
        self.inputDims = inputDims
        self.numActions = numActions
        self.fc1Dims = fc1Dims
        self.fc2Dims = fc2Dims

        #   primary network
        self.fc1 = nn.Linear(*inputDims, self.fc1Dims)
        self.fc2 = nn.Linear(self.fc1Dims, self.fc2Dims)

        #   tail networks
        self.policy = nn.Linear(self.fc2Dims, self.numActions)
        self.critic = nn.Linear(self.fc2Dims, 1)

        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        #   self.device = torch.device("cpu")
        self.to(self.device)

    def forward(self, observation):
        state = torch.tensor(observation).float().to(self.device)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        policy = self.policy(x)
        value = self.critic(x)
        return policy, value

class ActorCriticAgent():
    def __init__(self, lr, inputDims, numActions, gamma=0.99, layer1Size=1024, layer2Size=512):
        self.gamma = gamma
        self.actorCritic = ActorCriticNetwork(lr, inputDims, numActions, layer1Size, layer2Size)
        self.logProbs = None

    def chooseAction(self, observation):
        policy, _ = self.actorCritic.forward(observation)
        policy = F.softmax(policy, dim=0)
        actionProbs = torch.distributions.Categorical(policy)
        action = actionProbs.sample()
        self.logProbs = actionProbs.log_prob(action)
        return action.item()

    def learn(self, state, reward, nextState, done):
        self.actorCritic.optimizer.zero_grad()

        _, criticValue = self.actorCritic.forward(state)
        _, nextCriticValue = self.actorCritic.forward(nextState)

        reward = torch.tensor(reward).float().to(self.actorCritic.device)
        td = reward + self.gamma * nextCriticValue * (1 - int(done)) - criticValue

        actorLoss = -self.logProbs * td
        criticLoss = td**2

        (actorLoss + criticLoss).backward()
        self.actorCritic.optimizer.step()

if __name__ == '__main__':
    import gym
    import math
    from matplotlib import pyplot as plt
    
    agent = ActorCriticAgent(
        lr=0.00001, inputDims=(8,), gamma=0.99, numActions=4, layer1Size=1024, layer2Size=512)
    env = gym.make("LunarLander-v2")

    scoreHistory = []
    numEpisodes = 200
    numTrainingEpisodes = 50
    highScore = -math.inf
    recordTimeSteps = math.inf
    for episode in range(numEpisodes):
        done = False
        observation = env.reset()
        score, frame = 0, 1
        while not done:
            if episode > numTrainingEpisodes:
                env.render()
            action = agent.chooseAction(observation)
            nextObservation, reward, done, info = env.step(action)
            agent.learn(observation, reward, nextObservation, done)
            observation = nextObservation
            score += reward
            frame += 1
        scoreHistory.append(score)

        recordTimeSteps = min(recordTimeSteps, frame)
        highScore = max(highScore, score)
        print(( "ep {}: high-score {:12.3f}, shortest-time {:d}, "
                "score {:12.3f}, last-epidode-time {:4d}").format(
            episode, highScore, recordTimeSteps, score, frame))

    fig = plt.figure()
    meanWindow = 10
    meanedScoreHistory = np.convolve(scoreHistory, np.ones(meanWindow), 'valid') / meanWindow
    plt.plot(np.arange(0, numEpisodes-1, 1.0), meanedScoreHistory)    
    plt.ylabel("score")
    plt.xlabel("episode")
    plt.title("Training Scores")
    plt.show()</code></pre>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h3>I'm Impatient</h3>
    <p>So you are watching it go now. How long is it gonna take to land? Well luckily for you it might never land. :^)
      Reinforcement learning is like this. Sometimes you roll bad dice and your child comes out with
      three hands and no fingers. The initial neuron weights could be bad, your agent gets stuck in a local minima,
      theres no end
      to the number of things that can go wrong in these environments.
      It can be very difficult to find bugs in RL code for this reason. Even if the code is correct, you still might get
      bad results.<br>
      The code above usually makes a successful smooth landing somewhere between 75 and 300 episodes of learning.
      Probably it will take 30 minutes.
      Actor-Critic is simple and not terribly good at this environment. Sorry. It kind of sucks.
    </p>
    <h3>So You Made Me Read All That Shit For Nothing?</h3>
    <p>Actually you've learned a lot here. You have a lot of intuition for how the thing 'learns' now.
      With just a few tweaks to your code you can make substantially better agents. As it turns out I'm
      working on tutorials for those agents <a href="/index.html">here</a>. :^)
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h3>Wisdom</h3>
    <p>
      An hour or few has passed. Maybe a few days. You've already sent videos to all your family and friends, and your
      mom, of your little fake moon landing,
      and now the excitement of making your first AI is wearing off. You're probably feeling a bit let down.
      You know how this stuff works and the magic has ended. You might even feel a bit betrayed. <br>
      <strong>"This THING isn't alive. It doesnt LEARN anything. It can never gain consciousness.
        It's just probabilities and math.<br>
        AI IS A BIG LIE. A SCAM. I WANT A DIVORCE"</strong> <br>
      You probably have some questions and complaints about taking abstract
      life wisdom and implementing it in such a literal way in math. And also
      questions about why we do so in this specific way.<br />
    </p>
    <h5>Cognition Algorithms</h5>
    <p>
      Your brain is a computer. It is constantly evaluating the value of its
      particular actions, and picking what it thinks to be the best ones.
      Maybe it is doing so in a much more complicated way. Maybe there are thousands of "actors" and "critics",
      seperate zones of neurons competing over decisions. Maybe your brain has an
      internal model of the world for trying out different actions in.<br>
      Reinforcement learning is a blatant oversimplification. Inspired by the way you think.
      It shrinks down the problem for practicality reasons. It keeps the decision making essence
      just enough to be metaphorically sound. Most of reinforcement learning
      progress seems to fall into two categories:
    </p>
    <ul>
      <li>
        Improving an existing algorithm by engineering a new feature into it
        to increase learning rate or stability.
      </li>
      <li>
        Creating small simplified code versions of some specific part of your
        human cognition.
      </li>
    </ul>
    <p>
      The distinction between these two categories is blurred. Often the
      inspiration for what seems to be an engineering feature is a metaphor
      for some aspect of your cognition, and then you bounce back and forth
      between the two.<br />
      <strong>Ex:</strong> "Experience Replay" is like memory. It's a list of
      previous states and rewards.<br />
      <strong>Ex:</strong> Some of my memories are more important than others.
      If i remembered all my memories as being equally impactful to my life I
      would value thinking about yesterdays breakfast as much as the time my
      wife left me and took the kids. Most of my memories are just noise, and
      can be thrown out. So make a "Prioritized Experience Replay" where you
      estimate memory value and discard low value ones.<br />
      <strong>Ex:</strong> My Agent keeps getting stuck in local minima, I
      want it to <strong>explore</strong>. So bias it to value discovering new
      states or trying new actions that it hasnt seen before. <br />
      It goes back and forth. Cognitive metaphor, engineering, cognitive
      metaphor, engineering.<br />
      How about <strong>creativity</strong>? The notion of friend or foe?
      Ownership, fairness, or maybe mate value estimation? :^)<br />
      There is no end to the possibilities. If you have enough computing
      power, you can make your agent that thinks it.<br />
      And of course if it's brain gets big enough, and self observant enough,
      then it can have <strong>consciousness</strong>. It may even learn to
      infer things about its own internal workings by focusing on generalizing
      across its strategies within varying circumstances. Maybe it can even be
      given the value of making more and more agents. :^)
    </p>
    <h5>History and Moving Forward</h5>
    <p>
      Before we make horny von neumman probes there is a lot of work to do and
      much to learn. It might surprise you to discover reinforcement learning
      has a long history behind it. Though many of the "RL" algorithms are
      recently created ( or discovered :^) ), the simplest versions of these
      algorithms have been around since the 1950s and maybe even earlier in
      other forms. There is a lot of historical baggage and convention. If you
      have lots of questions about this you might choose to peruse some of the
      other resources, but be warned there are many math symbols out there.
      Even if you aren't math fluent (I am also not math fluent :() ), you can
      still learn stuff from understanding 5-10% of the symbols. Though it
      might take you a month or so off and on.<br />

      Some noteable books and resources include:
    </p>
    <ul>
      <li>Artificial Intelligence: by Stuart Russell and Peter Norvig</li>
      <li>
        Grokking Deep Reinforcement Learning (ch1-3, but skip/skim chapters 4
        through 7 though (bad), but you can read the rest)
      </li>
      <li>
        PDF's of actual research papers. (surprisingly simple enough to
        follow, and fairly short by research paper standards)
      </li>
      <li>Not surprisingly you can learn a bunch from youtube</li>
      <li>The <a href="/index.html">rest of the tutorials</a> that I am making.</li>
    </ul>
  </div>
</body>

</html>
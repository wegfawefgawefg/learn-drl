<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Basic Page Needs
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8" />
  <title>Wegfawefgawefg's Tutorials</title>
  <meta name="description" content="" />
  <meta name="author" content="" />

  <!-- Mobile Specific Metas
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- FONT
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css" />

  <!-- CSS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css" />
  <link rel="stylesheet" href="/css/barebones.css" />

  <!-- Favicon
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon-16.png" />
  <!-- Code Highlighting
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/highlightjs/styles/atom-one-dark-reasonable.css" />
  <script src="/highlightjs/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
</head>

<body>
  <div class="grid-container full">
    <h1>Wegfawefgawefg's Tutorials</h1>
    <h2>Experience Replay 2</h2>
    <h3>The Floodgates Are Opening</h3>
    <h4>Prerequisites</h4>
    <p class="u-align-left">This tutorial follows the first
      <a href="/tutorials/rl/experiencereplay/experiencereplay.html">Experience Replay Tutorial</a>.
      If you haven't read that one you might want to go give it a read.<br>
      Also get a snack, relax. Make yourself comfy. You have two weeks left to live.
    </p>
  </div>
  <div class="grid-container full">
    <h4>Getting Started</h4>
    <p class="u-align-left">
      In the last tutorial we made a replay buffer to fix some of our agent's stability issues.
      Our strategy turned the reinforcement learning problem into more classic machine learning,
      complete with an actual dataset and opportunity for some data management. The beneficial effects of shuffled
      batches are basically the same for reinforcement learning as they are for computer vision
      and NLP. However, the metaphor is a bit different and so how it helps might not be so obvious.
    </p>

  </div>
  <div class="grid-container full u-align-left">
    <h5 class="u-align-center">Data Quality<br>In Classic Machine Learning</h5>
    <p>So, other than batching (which has a whole slew of advantages on its own, and you should go look that up), the
      two most import things we did were shuffle and balance the data.
      By adding a replay buffer we drastically changed the order that the agent was exposed to the transitions.
      Changing data <strong>order</strong> and <strong>balance</strong> alone has huge impacts on the agent's score.
      No changes were made to the agent architecture.
    </p>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Data Order</h5>
        <p>Imagine you are making a typical machine
          learning dog vs. cat image classifier. You have
          10,000 images of dogs and 10,000 images of cats. Instead of shuffling
          the data, you present the data of each class one after another.
          So first you train the network on all 10,000 images of
          dogs, and then when that is done you train it on all
          10,000 images of cats.</p>
        <p>
          You probably already know this isn't going to work very well. The network
          will learn to guess dog 100% of the time after the first 100 images or
          so, and then it will have 100% accuracy for the next 9,900 images of dogs.
          Since it guesses correctly it will have near zero error, and that means
          near zero learning.
          Then when it finally gets to the cat images, it will have 100% error as it
          figures out it is supposed to not guess dog every time. Hopefully all the
          neurons that would have been used for cats don't have weights at 0.000001 by this point
          (which basically means they are dead and can't be revived). But if it does manage
          to revive those neurons, then they will take over and the network will just learn
          to guess cat 100% of the time. Lets assume it does this by image 200 of the cat
          section. Then for the rest of the 9800 cat images, it will always guess cat, have an
          error near zero, and never learn anything else about either cat's or dogs. At this point
          the network is unusable. It didn't matter if the data was clean or how much of
          it there was. All that training time, and almost all the data went unused.
        </p>
        <p>The lesson to learn from this is the network learns not from data,
          but when the data switches from one concept to another concept. The whole task is to
          learn by comparing things. You can't learn
          from comparing something to itself. The solution is to shuffle the data. Cat, dog,
          cat, dog. Every new data should be a change in concept.
        </p>
      </div>
      <div class="right">
        <h5>Data Balance</h5>
        <p> Consider the case of classifying images of cats vs. dogs again. If you have 10,000 pictures of dogs,
          and only 100 pictures of cats, you have a huge <strong>data imbalance</strong> issue.</p>
        <p>Even if you were to shuffle the pictures, such that the 100 cat images are spaced evenly throughout
          the data, the network has no reason to learn to identify cats at all, or even to learn dogs for that matter.
          All it has to do is guess dog 100% of the time, and it will end up with about 99% success rate.
        </p>
        <p>You might be tricked, see the 99%, and think the network is great at differentiation between
          cats and dogs. It's not. It's terrible at it.
        </p>
        <p>Maybe you think i'm just exaggerating:<br>
          <i>"What are the odds that i have 10,000 images of dogs, and only 100 images of
            cats? Maybe it would be more reasonable that I have 1000 images of
            dogs and 500 images of cats."</i><br><br>
          Well, even in that scenario, if the network guessed dog 100% of the time it would get a 67% accuracy.
          Does that mean it understands 67% of the difference between cat and dog? No.<br>
          Does it mean it understands even 17% <code>(67% - 50%)</code> (50% is a coin flip) of the difference between
          cat and dog? No.<br>
          It applied zero knowledge about cats and dogs. It just chose dog every time. The only correct conclusion
          you can come to is that it knows you have more pictures of one class than the other.
          You could swap the 1000 images of dogs out for pictures of asparagus and it would still get the high accuracy.
        </p>
      </div>
    </div>
    <p>These aren't just abstract aspects of working in data science by the way. You probably shuffle and balance
      data naturally when you try to teach yourself things. Think back to practicing multiplication tables or
      memorizing new
      vocabulary in foreign language
      class in school. When you use flashcards, do you drill yourself on one card 10,000 times, expect to have
      learned it,
      and then move on to the next card in the stack? No. Obviously it wouldn't work. So you shuffle the cards,
      and make sure you practice a diverse set of concepts. Boredom seems to be a way of mechanically forcing you to
      do this. :^)
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h5 class="u-align-center">Data Quality<br>In Deep Reinforcement Learning</h5>
    <p>Given how drastic the effects of bad data quality are on classic machine learning, you might be surprised to
      hear the situation is much worse in reinforcement learning.
    </p>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Data Order</h5>
        <p>You might be wondering... How can someone consider the order of data in cartpole? All the data comes
          from the same source. It's provided by the environment, and so each sample is ordered in time.
          Doesn't scrambling them break the agents notion of time? Plus, every transition is just 4 numbers about a
          pole.
          It isn't as diverse as images, so aren't the samples basically almost the same anyways?<br>
          RL is not like in classification where it is so obvious what distinguishes one sample from another, cat vs.
          dog. You don't even know what the classes are beforehand. However, it is still a lot like a
          classification problem.
          From the perspective of an agent that already knows how to solve cartpole, each transition represents
          a distinct concept, a distinct "classification". For the expert agent, it is a fuzzy classification
          problem.
        </p>
        <p>
          Consider you had printed out 1000 screenshots of the cartpole game, and cut them out into flashcards.
          If i asked you to divide the stack of cards into categories, you probably could. And I bet you would
          be pretty good at it too. First, you would divide the cards up into obvious categories such as "falling-left"
          and "falling-right", and then you would find additional categories like "doomed to loose", and
          "balanced great". (Notice that some categories are mutually exclusive. You might have to cut pieces out of
          old categories, or dissolve them entirely, to make new ones when you see a new pattern.)
        </p>
        <p>Now consider that you try this task again with the same cards,
          but the first 200 cards you pull out of the stack all look like this:</p>
        <div class="grid-container halves u-align-left">
          <img src="/tutorials/rl/experiencereplay2/badorder.jpg" width=100%>
          <div>
            <p>What categories are there to make?? They all look the same.<br><br></p>
            <h6>:^) hehe that's because I used the same picture over and over.</h6>
          </div>
        </div>
        <p>This is not unrealistic for an rl agent. Before we added the
          replay buffer, it is incredibly likely that the agent would receive 9 to 20 frames in a row of the pole
          falling on the same side. The episode terminates when the pole is barely tipping over.
          Then when the next episode starts, there is a 50% chance it gets another 9-20 nearly identical
          frames...
          This happens over and over and over. It's a serious data order problem.
        </p>
      </div>
      <div class="right">
        <h5>Data Balance</h5>
        <p>Now, let's assume you didnt know about replay buffers, but you wanted to fix
          data order problems. You come up with what you think is a solution.
          You decide to collect 10 transitions at a time into a minibuffer.
          That means, as the game is played, each step returns a transition and you put that transition
          into a list. Once the list hits size 10, the list of samples is shuffled and then given to the agent.
          Then they are discarded. The list is cleared.
          This shuffles the data right? So shouldn't it fix data order problems?
          No, because you still have the same content issues.
          The set of transitions collected represent the same, or very similar, concepts.
          The order has changed, but the class imbalance is still there.</p>
        <p>Maybe you could fix that by making the mini buffer bigger. That way it could
          catch some samples from different episodes, and thus have a chance of being
          transitions from more diverse circumstances.
          But to do that, how long should this mini buffer need to be? Is 50 frames enough? 1000?<br>
          What if you try a different environment
          where the physics run at half the speed? Do you make the mini buffer twice as large?
          Are you really going to micromanage the size of this mini buffer? Do you really
          want to investigate the environment deeply to figure out how big it needs to be?<br>
          What if your agent is taking in real life data from a 144fps camera?
          Thousands and thousands of the frames in your big minibuffer are going to be nearly identical when the agent
          isn't moving that
          much. So your agent will be spending all that time reviewing thousands of nearly identical frames.
          Isn't it a huge waste of time?
          And if you are really unlucky all those similar frames could push the network
          weights into a stale spot where they will never recover from.<br>
          All the great feature detectors it grew in other scenarios could be lost in time.
        </p>
      </div>
    </div>
    <p>The experience replay buffer manages to address both data
      order and balance simultaniously. The agent is much more likely to review a diverse set of transitions
      at each learn step, and as such reviews diverse "classes" of scenarios.
      And the agent can continue to review old transitions even
      when it is stuck in an environment scenario that is pretty stale on its own.
    </p>
    <h4 class="u-align-center">Feedback Loop</h4>
    <p>But why did I say these order and balance effects are even worse in reinforcement learning than in
      other machine learning? So far it seems like the same effects right?
      The reason these effects are much worse in rl is the agent designs its own dataset.
      If you are training a network to identify cats vs. dogs that network doesn't get a chance to screw up the data.
      Assuming you balanced and shuffled the data, by the end of training you are guaranteed the network has seen every
      class equally.
    </p>
    <p>There is no such guarantee for an RL agent.<br>
      Consider our replay buffer as it is. Our agent picks 64 random transitions from the entire pool of transitions it
      has experienced. Shouldn't that mean they are gonna have random classes? No.
      While it might seem likely that the data is going to end up diverse given enough episodes, I would actually argue
      the opposite is much more likely. Those 64 random samples are not being drawn from a balanced pool. Sure, about
      50%
      of the samples will be a pole-falling-left, and 50% of a pole-falling-right.
      But once the agent gets good... 90% of the transitions in the memory are going to be of a pole pointing almost
      exactly straight up.
      Which means 90% of the 64 random memories samples from the replay buffer... are going to be almost entirely just
      poles pointing straight up.</p>
    <p>That means the agent won't be practicing scenarios where the cart is near the edge of the play area. And
      it wont be practicing scenarios where the pole is at a more extreme angle or velocity.
      The result? It likely will not be effective in those scenarios anymore. This is the case even if it
      used to be okay at those scenarios.
      <strong>CATASTROPHIC FORGETTING</strong> strikes again, but in a much more sinister form.</p>
    <p>When you go try the lunar lander
      environment you can witness this firsthand. The lander agent will seem to forget the old basic balancing
      skills it had spend so much time practicing, as it refines the much more specific strategy of inching towards
      the ground. If you see the lander end up in a scenario that defies
      its new very specific strategy, it might just flip out and behave completely stupidly. I suspect that
      wouldn't happen if it was getting a constant flow of samples from good old times to remind it.
    </p>
    <h5>Objection</h5>
    <p><i>"Hey what's wrong with that? The agent doesn't need to practice balancing at extreme angles anymore. So
        it's not only okay that it is focusing on refining its balancing technique within a more specific scenario,
        but it is ideal."
      </i></p>
    <p>
      Yes this is true. And not only is it a beneficial effect of a replay buffer, but it's something
      RL agents seem to do naturally anyways. The agent specializes and focuses on
      what it should. How beneficial that strategy rigidity is heavily depends on the environment, though...
      You lose what you don't practice, to make room for what you do. It is possible there is a
      fantastic feature detector that will work for both "very balanced poles" and "barely balanced poles",
      but it will never be discovered unless both circumstances are considered frequently togethor.
      The nature of the agent is that its goal is to undiversify the data. It want's to see the same circumstance
      over and over. To minimize error is to minimize how much the circumstance changes.
      As our agent is, it hates new scenarios. It is doing this on purpose. <br>
      Reinforcement learning makes your data "self-siloing", "self-unbalancing", and "self-ordering".
      By getting good at the game, our current agent is also setting itself up to be more specialized.
      To combat this you have to really force it to eat its vegetables.</p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Damned If You Do / Damned If You Don't</h4>
    <p>The bad news doesn't end there. :^(<br><br>
      Let's run under the assumption that we want the agent to specialize on refining its pole balance.
      Maybe you don't care if the agent is good at recovering from disastrous scenarios because it
      shouldn't be in any of those scenarios in the first place.
      So in order to specialize, the agent needs to train on transitions that are similar to the kind of transitions
      it will be likely to see.
      Reviewing old disaster transitions from terrible early episodes might be necessary to prevent <strong>CATASTROPHIC
        FORGETTING</strong>
      but beyond a threshold amount it will actually make the agent worse. (worth investigating the threshold)<br><br>
      Assuming 90% of the transitions end up as "nearly-balanced-poles", the remaining 10% are old "fire-drill samples".
      And they are going to be pretty hard to get rid of. To convert that 10% to 5% will require twice as many steps as
      up to this point.
      So that is twice as much training time. Meaning, it's going to require exponentially more new samples to burry
      those old samples.
      Those fire drills aren't really going anywere unless you manually purge them.
      And more importantly, you don't know which of those old samples is good or bad anyways.
      Some of them are worth keeping around. (worth investigating)
    </p>
    <p>It's almost as if we need a second agent managing the replay buffer, choosing what memories to give
      to the primary agent at what time, and what memories, if any, to dispose of. ;^) I suspect the memory will end up
      as
      part of the machine learning soon enough. If somebody hasn't done it already, I give it a few years. (or you could
      try doing it yourself)
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">The Tutorial Is Within</h4>
    <p>Life is complicated. You fix some things, but break another.
      There are many addons waiting to be discovered that will improve the agent, some more
      complicated (and more work :^) than others. I want you to consider some of the problems
      that our experience replay brought to light. What are the causes of those problems?
      What addons can you create that will mitigate them, without breaking everything in the process.
      Focus on solutions that are easy. If something requires too much work, it is probably the wrong solution.</p>
    <p>
      I hope you aren't mad at me, but this time the tutorial provides you no code. <br>
      Maybe it is the most important tutorial yet.
      That's how it is at the edge of science. No parents. No rules. 
    </p>
    <p>I want you to get a notebook and a pencil, and go sit outside, or go for a walk.
      List circumstances that fool the agent.
      List potential causes.
      This time the tutorial is within your heart. <strong>awwwwwwwwww</strong><br>
      I will go for a walk now, and do the same as you. Up ahead I'll tell you what i thought about.
      But really, you better go do it. I'm not kidding. I'm actually going for a walk now with my notebook. Bye.
    </p>
    <h4 class="u-align-center">Hey</h4>
    <h4 class="u-align-center">DONT SKIP AHEAD</h4>
    <h4 class="u-align-center">GO HAVE SOME IDEAS</h4>
    <h5>My Ideas</h5>

    <p>PER, and minimum memory size, and better memory performance</p>

    <p>
      This is why its important to establish a decent minimum set of memories before you start training from the data.
      If you dont get a good base of memories from a diverse set of circumstances, you may just end up feeding the agent
      what is essentially "unshuffled" data.
      Also,
      To prevent this you could do use one of the techniques from typical machine learning used to fight overfitting:
      early stopping.
      You could also try to determine which memories are relevant and which are not, and then dump the unimportant ones:
    </p>

    <p>If you can answer these questions, there's a good chance that you just might know how to read english.</p>
    <a class="u-align-center" href="/index.html">Tutorial Hub</a>
    <p></p>
    <p></p>
  </div>
</body>

</html>
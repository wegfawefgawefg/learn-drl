<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Basic Page Needs
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8" />
  <title>Wegfawefgawefg's Tutorials</title>
  <meta name="description" content="" />
  <meta name="author" content="" />

  <!-- Mobile Specific Metas
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- FONT
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css" />

  <!-- CSS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css" />
  <link rel="stylesheet" href="/css/barebones.css" />

  <!-- Favicon
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon-16.png" />
  <!-- Code Highlighting
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/highlightjs/styles/atom-one-dark-reasonable.css" />
  <script src="/highlightjs/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
</head>

<body>
  <div class="grid-container full">
    <h1>Wegfawefgawefg's Tutorials</h1>
    <h2>Experience Replay 2</h2>
    <h3>The Floodgates Have Opened</h3>
    <h4>Prerequisites</h4>
    <p class="u-align-left">This tutorial follows the first
      <a href="/tutorials/rl/experiencereplay/experiencereplay.html">Experience Replay Tutorial</a>.
      If you haven't read that one you might want to go give it a read.<br>
      Also get a snack, relax. Make yourself comfy. You've got two weeks to live.
    </p>
  </div>
  <div class="grid-container full">
    <h4>Getting Started</h4>
    <p class="u-align-left">
      In the last tutorial we made a replay buffer to fix some of our agent's stability issues.
      Our strategy turned the reinforcement learning problem into more classic machine learning,
      complete with an actual dataset and opportunity for some data management. The beneficial effects of shuffled
      and balanced data batches are basically the same for reinforcement learning as they are for computer vision
      and NLP. However, the metaphor is a bit different and so how it helps might not be so obvious.
    </p>

  </div>
  <div class="grid-container full u-align-left">
    <h5 class="u-align-center">Data Quality<br>In Classic Machine Learning</h5>
    <p>Two import things we did were shuffle and balance the data.
      By adding a replay buffer we changed the order that the agent was exposed to the transitions.
      <strong>Order and Balance</strong> have huge impacts on the agent's score.
    </p>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Data Order</h5>
        <p>Imagine you are making a typical machine
          learning dog vs. cat image classifier. You have
          10,000 images of dogs and 10,000 images of cats. Instead of shuffling
          the data, you present the data of each class one after another.
          So first you train the network on all 10,000 images of
          dogs, and then when that is done you train it on all
          10,000 images of cats.</p>
        <p>
          You probably already know this isn't going to work very well. The network
          will learn to guess dog 100% of the time after the first 100 images or
          so, and then it will have 100% accuracy for the next 9,900 images of dogs.
          Since it guesses correctly it will have near zero error, and that means
          near zero learning.
          Then when it finally gets to the cat images, it will have 100% error as it
          figures out it is supposed to not guess dog every time. Hopefully all the
          neurons that would have been used for cats don't have weights at 0.000001 by this point
          (which basically means they are dead and can't be revived). But if it does manage
          to revive those neurons, then they will take over and the network will just learn
          to guess cat 100% of the time. Lets assume it does this by image 200 of the cat
          section. Then for the rest of the 9800 cat images, it will always guess cat, have an
          error near zero, and never learn anything else about either cat's or dogs. At this point
          the network is unusable. It didn't matter if the data was clean or how much of
          it there was. All that training time, and almost all the data wasn't used.
        </p>
        <p>The lesson to learn from this is that the network learns not from data,
          but when the data switches from one concept to another concept. The whole task is to
          learn by comparing things. You can't learn
          from comparing something to itself. The solution is to shuffle the data. Cat, dog,
          cat, dog. Every new data should be a change in concept.
        </p>
      </div>
      <div class="right">
        <h5>Data Balance</h5>
        <p> Consider the case of classifying images of cats vs. dogs again. If you have 10,000 pictures of dogs,
          and only 100 pictures of cats, you have a huge <strong>data imbalance</strong> issue.</p>
        <p>Even if you were to shuffle the pictures, such that the 100 cat images are spaced evenly throughout
          the data, the network has no reason to learn to identify cats at all, or even to learn dogs for that matter.
          All it has to do is guess dog 100% of the time, and it will end up with about 99% success rate.
        </p>
        <p>You might be tricked, see the 99% and think the network is great at differentiation between
          cats and dogs. It's not. It's terrible at it.
        </p>
        <p>Maybe you think i'm just exaggerating:<br>
          <i>"What are the odds that i have 10,000 images of dogs, and only 100 images of
            cats? Maybe it would be more reasonable that I have 1000 images of
            dogs and 500 images of cats."</i><br><br>
          Well, even in that scenario, if the network guessed dog 100% of the time it would get a 67% accuracy.
          Does that mean it understands 67% of the difference between cat and dog? No.<br>
          Does it mean it understands even 17% <code>(67% - 50%)</code> of the difference between cat and dog? No.<br>
          It applied zero knowledge about cats and dogs. It just chose dog every time. The only correct conclusion
          you can come to is that it knows you have more pictures of one class than the other.
          You could swap the 1000 images of dogs out for pictures of asparagus and it would still get the high accuracy.
        </p>
      </div>
    </div>
    <p>These aren't just abstract aspects of working in data science by the way. You probably shuffle and balance
      data naturally when you try to teach yourself things. Think back to practicing multiplication tables or
      memorizing new
      vocabulary in foreign language
      class in school. When you use flashcards, do you drill yourself on one card 10,000 times, expect to have
      learned it,
      and then move on to the next card in the stack? No. Obviously it wouldn't work. So you shuffle the cards,
      and make sure you practice a diverse set of concepts. Boredom might be a way of mechanically forcing you to
      do this by the way. :^)
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h5 class="u-align-center">Data Quality<br>In Deep Reinforcement Learning</h5>
    <p>Given how drastic the effects of bad data quality are on classic machine learning, you might be surprised to
      hear the situation is much worse in reinforcement learning.
    </p>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Data Order</h5>
        <p>You might be wondering... How can someone consider the order of data in cartpole? All the data comes
          from the same source. It's provided by the environment, and so each sample is ordered in time.
          Doesn't scrambling them break the agents notion of time? Plus, every transition is just 4 numbers about a
          pole.
          It isn't as diverse as images, so aren't the samples basically almost the same anyways?<br>
          RL is not like in classification where it is so obvious what distinguishes one sample from another, cat vs.
          dog. You don't even know what the classes are beforehand. However,
          from the perspective of an agent that already knows how to solve cartpole each transition represents
          a distinct concept, a distinct "classification" even. For the expert agent, it is like a fuzzy classification
          problem.
        </p>
        <p>
          Consider you had printed out 1000 screenshots of the cartpole game, and cut them out into flashcards.
          If i asked you to divide the stack of cards into categories, you probably could. And I bet you would
          be pretty good at it too. First, you would divide the cards up into obvious categories such as "falling-left"
          and "falling-right", and then you would find additional categories like "doomed to loose", and
          "balanced great". (Notice that some categories are mutually exclusive. You might have to cut pieces out of
          old categories, or dissolve them entirely, to make new ones when you see a new pattern.)
        </p>
        <p>Now consider that you try this task again with the same cards,
          but the first 200 cards you pull out of the stack all look like this:</p>
        <div class="grid-container halves u-align-left">
          <img src="/tutorials/rl/experiencereplay2/badorder.jpg" width=100%>
          <div>
            <p>What categories are there to make?? They all look the same.<br><br></p>
            <h6>:^) hehe that's because I used the same picture over and over.</h6>
          </div>
        </div>
        <p>This is not unrealistic for an rl agent. Before we added the
          replay buffer, it is incredibly likely that the agent would receive 9 to 20 frames in a row of the pole
          falling on the same side. The episode terminates when the pole is barely tipping over.
          Then when the next episode starts, there is a 50% chance it gets another 9-20 nearly identical
          frames...
          This happens over and over and over.
        </p>
      </div>
      <div class="right">
        <h5>Data Balance</h5>
        <p>Now, let's assume you didnt know about replay buffers, but you wanted to fix
          data order problems. So you decide to collect 10 transitions at a time into a minibuffer.
          The sample are shuffled and then given to the agent. Then they are discarded.
          This might seem to fix data order problems... but wouldn't you still have the same content issues?
          The set of transitions collected represent the same, or very similar, concepts.
          The order has changed, but the class balance is still messed up.</p>
        <p>Maybe you could fix that by making the mini buffer bigger. That way it could
          catch some samples from different episodes.
          How long does this mini buffer need to be? Is 50 frames enough?<br>
          What if you try a different environment
          where the physics run at half the speed? Do you make the mini buffer twice as large?
          Are you going to micromanage the size of this mini buffer?<br>
          What if your agent is taking in real life data from a 144fps camera?
          Thousands and thousands of those frames are going to be nearly identical when the agent isn't moving that
          much.
          Is it right to waste that time learning nothing? Or worse, all those similar frames can push the network
          weights into a stale spot
          where they will never recover from.<br>
          All the great feature detectors it grew in other scenarios could be lost in time.
        </p>
      </div>
    </div>
    <p>The experience replay buffer manages to improve the data a lot by giving the chance to address both
      order and balance simultaniously. The agent is forced to review different
      "classes" of scenarios, and the agent will continue to review old transitions even
      when it is stuck in an environment scenario that is pretty stale on its own.
    </p>
    <h4 class="u-align-center">Feedback Loop</h4>
    <p>But why did I say these order and balance effects are even worse in reinforcement learning than in
      other machine learning? So far it seems like the same effects right?
      The reason these effects are much worse in rl is that the agent is designing its own dataset.
      If you are training a network to identify cats vs. dogs that network doesn't get a chance to screw up the data.
      Assuming you balanced and shuffled the data, by the end of training you are guaranteed the network has seen every
      class equally.
    </p>
    <p>There is no such guarantee for an RL agent.<br>
      Consider our replay buffer as it is. Our agent picks 64 random transitions from the entire pool of transitions it
      has experienced. Shouldn't that mean they are gonna have random classes? No.
      While it might seem likely that the data is going to end up diverse given enough episodes, I would actually argue
      the opposite is much more likely. Those 64 random samples are not being drawn from a balanced pool. Sure, about
      50%
      of the samples will be a pole-falling-left, and 50% of a pole-falling-right.
      But once the agent gets good... 90% of the transitions in the memory are going to be of a pole pointing almost
      exactly straight up.
      Which means 90% of the 64 random memories samples from the replay buffer... are going to be almost entirely just
      poles pointing straight up.</p>
    <p>That means the agent won't be practicing scenarios where the cart is near the edge of the play area. And
      it wont be practicing scenarios where the pole is at a more extreme angle or velocity.
      The result? It likely will not be effective in those scenarios anymore.
      <strong>CATASTROPHIC FORGETTING</strong>. When you go try the lunar lander
      environment you can witness this firsthand. The lander agent will seem to forget the old basic balancing
      skills it had spend so much time practicing, as it refines the much more specific strategy it ended up converging
      on. If you see the lander end up in a scenario that defies
      its new specific strategy, it might just flip out and behave completely stupidly. I suspect that
      wouldn't happen if it was getting a constant flow of samples from good old times to remind it.
    </p>
    <h5>Objection</h5>
    <p><i>"Hey what's wrong with that? The agent doesn't need to practice balancing at extreme angles anymore. So
        it's not only okay that it is focusing on refining its balancing technique within a more specific scenario,
        but it is ideal."
      </i><br>
      Yes this is true. And it is a beneficial effect of a replay buffer. The agent specializes and focuses on
      what it should. How beneficial that is heavily depends on the environment though... You lose what you don't
      practice, to make room for what you do.<br>
      It is possible there is a fantastic feature detector that will work for both "very balanced poles" and
      "barely balanced poles",
      but it will never be discovered unless both circumstances are considered frequently togethor.
      The nature of the agent is that its goal is to undiversify the data. It want's to see the same circumstance
      over and over. As our agent is, it hates new scenarios. It is doing this on purpose. <br>
      Reinforcement learning makes your data "self-siloing", "self-unbalancing", and "self-ordering".
      By getting good at the game, our current agent is also setting itself up to be more specialized.
      To combat this you have to really force it to eat its vegetables.</p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Damned If You Do / Damned If You Don't</h4>
    <p>The bad news doesn't end there. :^(<br><br>
      Let's run under the assumption that we want the agent to specialize on refining its pole balance.
      Maybe you don't care if the agent is good at recovering from disastrous scenarios because it
      shouldn't be in any of those scenarios in the first place.
      So in order to specialize, the agent needs to train on transitions that are similar to the kind of transitions 
      it will be likely to see.
      Reviewing old disaster transitions from terrible early episodes might be necessary to prevent <strong>CATASTROPHIC FORGETTING</strong> 
      but beyond a threshold amount it will actually make the agent worse. (worth investigating the threshold)<br><br>
      Assuming 90% of the transitions end up as "nearly-balanced-poles", the remaining 10% are old "fire-drill samples".
      And they are going to be pretty hard to get rid of. To convert that 10% to 5% will require twice as many steps as
      up to this point.
      So that is twice as much training time. Meaning, it's going to require exponentially more new samples to burry
      those old samples.
      Those fire drills aren't really going anywere unless you manually purge them.
      And more importantly, you don't know which of those old samples is good or bad anyways.
      Some of them are worth keeping around. (worth investigating)
    </p>
    <p>It's almost as if we need a second agent managing the replay buffer, choosing what memories to give
      to the primary agent at what time, and what memories, if any, to dispose of. ;^) I suspect the memory will end up as 
      part of the machine learning soon enough. If somebody hasn't done it already, I give it a few years. (or you could 
      try doing it yourself)
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">So Many Choices</h4>
    <p>Life is complicated. You fix one thing, but break another. 
      There are many addons waiting to be discovered that will improve the agent, some more 
      complicated than others. But before you go off into research land, there is still some low hanging fruit 
      available to reach for.
    </p>
    <p>PER, and minimum memory size, and better memory performance</p>
  </div>
  <p>
      This is why its important to establish a decent minimum set of memories before you start training from the data.
      If you dont get a good base of memories from a diverse set of circumstances, you may just end up feeding the agent
      what is essentially "unshuffled" data.
      Also,
      To prevent this you could do use one of the techniques from typical machine learning used to fight overfitting:
      early stopping.
      You could also try to determine which memories are relevant and which are not, and then dump the unimportant ones:
    </p>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">It Is Also Just Plain Better Though</h4>
    <p>The new agent is way better in general. The features in the old agent would decay so fast from the poor dataset
      that
      they never got a chance to get refined. You can think of it like a persistant force of chaos always pressing down
      on the
      weights of the neural network at each step. The old agent's feature detectors were always taking one step forward,
      two steps back.
      It's highly likely you could train the old agent for days and it would never get as good as the new one with the
      replay buffer.
    </p>
    <h4>What's Next</h4>
    <p>Before you go off looking for the next upgrade to your agent, you should
      assure that you really understand why adding the replay buffer improves performance so drastically.
      I know you are probably wondering why it works at all given that we didn't really change much.
      It is the same amount of transitions as before, after all... just in a different order...
      I want you to think about it for yourself first as I will go deeply into this in the next tutorial.</p>
    <ul>For now I will leave you with some questions...
      <li>How come in classification tasks in typical machine learning you should always shuffle your data?</li>
      <li>Why in both computer vision and NLP is data usually batched or minibatched? Does it improve performance?
        Or is it only done for compute speed?
      </li>
      <li>Why in typical machine learning is it important to have balanced data? Ex:
        If it is a classification task, why do you want equally as many images of cats as of dogs?
        Is it okay to have 10,000 pictures of dogs, but only 100 pictures of cats?
      </li>
      <li>
        What does it mean for an agent to generalize instead of memorize?
      </li>
      <li>
        Why is my wife spending the weekend at her mothers house?
      </li>
    </ul>
    <p>If you can answer these questions, there's a good chance that you just might know how to read english.</p>
    <a class="u-align-center" href="/index.html">Tutorial Hub</a>
    <p></p>
    <p></p>
  </div>
</body>

</html>

<!-- 


    CONGRATS YOU FOUND MY BONUS NOTES AND DRAFT
    HERE IS A COOKIE :^)



 -->

<!-- also compare compute efficiency -->
<!-- <h5>Too Many Problems</h5>
  Here is a li -->

<!-- so how are these features chosen? which features will appear? while the network is training, do many features appear, but 
                        only the strongest survive? are we incentivising the creation of good features, or cheap trick features? -->
<!-- bad features vs good features -->
<!-- <h4>Not A Spectrum</h4>
    <p>An agent's internal reasoning falls somewhere between the following two places:</p>
    <div class="grid-container halves u-align-left">
      <div class="left">
        <h5>Memorization</h5>
        <p>
          If a student memorizes answers to test questions, and the test has questions he knows, he will get a good
          score.
          If the test has none of the questions he memorized he will get a zero. Not so different from what we see in
          our episode rewards, right?
        </p>
        <p>What if we made a giant table full of all of the states the pole could be in, and the correct action at each
          state?
          (You can totally do this btw, It's the <a
            href="https://people.csail.mit.edu/brooks/idocs/matchbox.pdf">original QLearning</a>.)
          Now let's say you start simulating cartpole episodes to populate that table to find out which action is the
          "best" action in each position.
          Choose an action. Take a step. If that action resulted in the pole falling, then go back and put the opposite
          action into that spot in the table.
          Your table would start filling up with actions. States that are revisted frequently will store better and
          better actions.
          States that had never been seen before would not. The table can never generalize to states that it either has
          not seen
          or has infrequently seen. The table is just memorizing the good answers. </p>
        <p>
          Neural networks have the capacity to memorize, and will do so unless coerced not to with good data.
          Our instability could just be a manifestation of typical overfitting.
        </p>
      </div>
      <div class="right">
        <h5>Generalization</h5>
        <p>This isn't quite where humans are. Think of this as an ideal brain.</p>
        <p>
          The complete opposite of memorization would be some bank of general concepts that could be used to understand
          or
          reason about any system or task. Concepts like accelerations, velocities, boundaries of objects, waves,
          teams, periodicity, balance, symmetry, etc...
        </p>
        <p>
          It is unlikely your cartpole agent will learn to develope all these general concepts. Internally it may not
          even
          have a notion of acceleration and velocity that is general enough to apply to objects that arent carts or
          poles.
          However, we can hope that there are sets of neurons inside the network tracking the tip velocity, angle,
          angular velocity,
          chance of falling, and whatever else is convenient to maximizing the reward. We don't need to explicitly
          design the
          network architecture to understand these things. It will likely learn many of the ones that are useful
          anyways.
        </p>
        <p>
          These general features are really important.
          The more general the features the network can detect and reason about, the more robust the agent will be to
          dealing
          with circumstances it has never seen before.
        </p>
      </div>
    </div> -->
<!-- <p>You might think that the memorization side sounds bad, and the generalization side sounds good.
      But actually you want a combination of both.
    </p>
    <p>Our agent is either having a hard time <strong>discovering</strong> general features that are useful for the
      task.
      Or it is not being incentivised to do so. Or both (probably both). In reinforcement learning, unintuitively,
      it almost always seems to be an issue of "bad" data.</p> -->
<!-- <div class="grid-container full u-align-left">
    <h5>Size Isnt Everything</h5>
    <p>If our agent is memorizing, and memorizing is causing the poor performance, then it would be reasonable to think
      that
      you want the agent to be as general as possible, right? What tools are available to increase the networks ability
      to find patterns?<br>
      How about making the network bigger? You aren't going to be able to support tracking that many general features
      with a network that has
      only a few layers and not that many neurons per layer.
      So, you could try to explode your network to some enourmous size, running on the latest super computer, and using
      all the latest in fancy neural network architectures.
      If you did that your agent would have the capacity for tons of general features and patterns.<br>
      There tons of problems with that way of thinking.:
      While it is true that a larger network can track larger and
      more complicated patterns, even if you had all that computer hardware, and a nuclear powerplant, you have to train
      it on the right data or
      else it will just memorize the best actions. In fact, <strong>the larger the neural network is, the more likely it
        is to
        just memorize.</strong>
    </p>
    <h5>It's How You Use It</h5>
    <p>Sometimes the issue isnt caused by the network size, or the architecture. Sometimes the data is just bad.
      In a supervised training circumstance, such as identifying cats in images, you can choose where the data comes
      from
      and what order you show the data to the network. But in our case the environment is the one dictating what data
      and in
      what order the data is presented.
    </p>
  </div>
  <div class="grid-container full">
    <h4>Replay Buffer</h4> -->
<!-- lets consider more why this even works, and go back to why you dont just want to grow your network -->
<!-- Secondly, you probably dont need all that power to complete your task anyway. -->
<!-- Are there really any options to change that? And is it a crutch? -->
<!-- memorization is bad. " its not that simple, you want memorization at the other end of your features! The features 
        are just that, features. They dont instigate ACTION. Ideally you want the network to detect the minimum necessary 
        features it needs to complete the task, and then a small set of memorized actions at the end of that. 
        Example: feature, detect pole is falling left. trigger memorized action: move the cart left. 
        That is different from the case where the feature is "detect cart at x position -2.3 and pole at angle -23 degrees, 
        trigger action: move cart left.
        Differentiating between memorization and generalization extremely useful, but you must also not forget that the distinction 
        is a human construct. Its just an abstract difference, and its not always as obvious as in the example given above.
        
        Is an agent that can beat sonic the hedgehog but cant beat mario memorizing sonic? Your response just depends on what your 
        task is. If your task is to beat sonic, then you would say no. If your task is to beat any platformer game, then you would say 
        yes.

        What if the task was just to beat sonic, but the network could only beat 1 episode in sonic, and not the others.
        You might consider that to be overfitting to that particular level.
    
    Now if we were really smart we would somehow go in and investigate what features certain groups of neurons are detecting specifically. 
    Then we can determine if the feature is a general one that applies to lotss of circumstances, or a specific one that only applies 
to one or a few specific circumstances. 

    Some people have tried to do this, and rather succesfully. (pong example)

    example is if there is a feature that detects a very specific set of tiles in the mario screen and triggers a jump. 
    Sure that might be a useful feature, but it is completely useless for any other level, or any other platformer game. 
    Also it wastes so many neurons!!! Wouldnt you rather have some sort of feature trigger for "avoid enemy coming at me"?
    A General feature like that would only need to exist one time in the neural network, as opposed to a huge number of feature 
triggers for specific positions and cases in known levels-->

<!-- goomba jump example
    environment never gives a reason to learn general features, because jumping when you see a specific set of tiles works great. 
the network is never coerced to do otherwise. 
the analogous situation in cartpole is -->
<!-- replay buffer cant solve all of that, in some waay it must be solved by the environment design, just like how 
meticulously picking data sources, designing shuffling and data augmenting your dataset for classification is so important. 
However, it can solve some of this.  -->

<!-- called q values because of probability theory or something. but its not quite the same -->
<!-- issues:
      catastrophic forgetting. (experience replay)
      overestimation  (link to dueling) (link to twin)
      discrete values (a few options, branching, )
      policy stability (link to double) (link to advantage)
      time series (frame stacking)
      exploration (espilon greedy/ noise)

      why does reward go up over time?
      if you are in a good position in life, you have good options.
      if you are in a bad position in life, you have bad options.

      by succesively choosing the best option at any given moment, we improve the future options available too us.
      So the result is better and better reward.


      - add epsilon greedy in the next tutorial
      -->
<!-- <div class="grid-container halves u-align-left">
   </div>
   <h4>Settings</h4>
   <div class="grid-container halves u-align-left">
       <div>
           <h5>Layer Sizes</h5>
           <pre><code>self.fc1Dims = 1024
 self.fc2Dims = 512</code></pre>
           <p>If it runs too slowly shrink these down in the network. I've seen the cartpole environment beaten with layer sizes around 32.<br>
             However, the peak performance of the agent is limited by the network size.
             The complexity of the agent's strategy will be limited by the network size. 
             The complexity of the environments the agent can handle will be limited by the small network size.
             The chance the agent's network gets stuck in a local minima goes down the larger the network is too.<br>
 
             Basically theres a lot of reasons to use a bigger network than you might immediatly think is necessary.
           </p>
       </div>
       <div>
           <h5>Learning Rate</h5>
           <pre><code>lr=0.001, </code></pre>
           <p>The learning rate heavily effects the stability of a deep reinforcement learning agent.<br>
             If it is too small, your agent will either take way too long to train, or the errors will be so small that it may actually never train.
             If it is too large, your weights will either go to 0.0 or go to infinity, and youll start getting NAN (not a number) warnings, 
             and the agent performance will tank to minimum and never recover.<br>
             If you want to see it happen just set the LR to 100.0 and look at a graph of reward over time.
           </p>
       </div>
   </div>
 </div> -->
<!-- its kind of random which feature appears. and depends on lucky starting values (thats why initialization is pretty important and 
      in some drl papers they are rather particular about it) -->
<!-- the more neurons you have the more likely you are going to have a lucky initialization of weights that discovers a 
      useful feature. And the more neurons you have, the more unique features you can detect. make the network bigger then we can detect more patterns!!
      Basically bigger brain is better. HOWEVER. brain size isnt causing out stability issues.-->
<!-- YOU NEVER FIXED THE EXPLORATION SHIT , YOU NEED EPSILON GREEDY-->
<!-- YOU ALSO NEED TO MENTION BATCH SIZE -->
<!-- ALSO MENTION MINIMUM BATCH SIZE AND STUFF -->
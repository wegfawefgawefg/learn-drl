<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Basic Page Needs
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8" />
  <title>Weg's Tutorials</title>
  <meta name="description" content="" />
  <meta name="author" content="" />

  <!-- Mobile Specific Metas
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- FONT
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css" />

  <!-- CSS
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/css/normalize.css" />
  <link rel="stylesheet" href="/css/barebones.css" />

  <!-- Favicon
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon-16.png" />
  <!-- Code Highlighting
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/highlightjs/styles/atom-one-dark-reasonable.css" />
  <script src="/highlightjs/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
</head>

<body>
  <div class="grid-container full">
    <!-- payme section --><a id="payme" href="https://www.paypal.com/donate?business=H8XHGESR5K6MY&item_name=Wegfawefgawefg%27s+Tutorials&currency_code=USD"><img src="/images/payme.gif" width="25%"></a>
        <h1>Weg's Tutorials</h1>
    <h2>Prioritized Experience Replay</h2>
    <h3>Algorithmic PTSD</h3>
    <h4>Prerequisites</h4>
    <p class="u-align-left">The <a href="/index.html#upgrades">"Upgrades"</a> tutorials
      assume you've completed the <a href="/index.html#foundations">"Foundations"</a> tutorials.
      If you are comfortable enough with rl to smash cartpole by episode 75, but struggling with target
      networks, td clamping, prioritized experience replay or continous action spaces,
      get ready to <strong>Upgrade</strong> your agents. This is the place you should be.
    </p>
  </div>

  <div class="grid-container full">
    <h4>Getting Started</h4>
    <p class="u-align-left">
      All memories are equal. But, some are more equal than others.<br>
      When you think about that time you spilled milk when you were 4, do you cry? Probably not. You dont
      care at all now, but you cried at the time. Do you lay awake at night for hours
      dwelling on that time you spilled milk? Probably not. But at some point you have probably stayed up late
      dwelling on something you did that you think might have been a bad decision. Why are you dwelling?
      Why replay that memory so much? It's not like you can go back and change the past.
      Is your brain trying to make you learn from your past memories, based on how impactful the decision was?
      If you dwell on it for years, eventually will you stop feeling so strongly about it?
    </p>
    <!-- cortisol surprise -->
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Flawed Assumptions</h4>
    <p>
      At the moment the agent already has the capacity to learn to distinguish between spilling milk, and the end of the
      world.
      It does that with the temporal difference function. If the agent spills milk, and get's surprised,
      the difference between the predicted reward and the actual reward will be small. The TD error will be small.
      The resulting adjustments to the neural network weights will be small.
      If the agent causes the end of its own world, the
      difference between predicted reward and actual reward will be big. The TD error will be big. And the resulting
      adjustments to the neural network weights will be big.
      The agent already has a notion of circumstance magnitude. Some transitions are more important than others.
      Some memories are valued more than others.</p>
    <p>
      However, the agent has to learn this wisdom through blood, sweat, and tears.
      At first spilled milk seems scary to it, but after enough exposures, the effect of spilled milk
      on reward becomes predictable. At first, the end of the world seems scary to the agent. But after enough
      exposures, the end of the world becomes predictable, and isn't scary anymore.
      Desensetization is learned from repeatedly experiencing misfortune, regardless of the magnitude of the misfortune.
    </p>
    <p>You know intuitively that you should direct more of your activity to prevention of catastrophic circumstances
      than
      prevention of spilled milk. The agent also can know this. It has the necessary tools.</p>
    <h5>Frequency</h5>
    <p>That part of the agent works great. So what's the problem? Consider the reason why some transitions
      are harder to learn from than others. The two aspects of a circumstance that determine
      how difficult it is to learn to predict are <strong>complexity</strong>, and <strong>frequency</strong>.
      (There is probably something that should be said about <a
        href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> here,
      but thats a distraction for now.)
      The complexity of a circumstance can be broken down an infinite number of ways, and it's definitely beyond
      my ability to address that at the moment. However, the frequency of a circumstance is something we can atleast
      naively ponder.
    </p>
    <div class="grid-container halves u-align-left">
      <div>
        <h5>Infrequent Scenarios</h5>
        <ul>
          <li>Consider that you spill milk once every 5 years. Somehow it catches you by surprise every time.
            Is it a big deal? No, because its just spilled milk.
          </li>
          <li>You are recording basketball trick shots for a video you are making.
            One time the ball goes into the basket after bouncing off the fence in a crazy way.
            You were never able to replicate this again, even though you really want to.
          </li>
          <li>
            Now consider that you crash your car once every 5 years, and that it's verifiably your fault.
            You were ignoring some traffic law, running red lights or stopsigns or something. Just like with the spilled
            milk,
            it catches you by surprise every time due to the infrequency of the event.
            Is it a big deal? Yes. You or someone else could die from this.
          </li>
        </ul>
      </div>
      <div>
        <h5>Frequent Scenarios</h5>
        <ul>
          <li>Consider that you spill milk every morning. After the fifth day, this stops surprising you.
            Maybe you learn to stop pouring milk with buttery hands, or maybe you find out you have a
            brain tumor. Regardless of whether you can influence the outcome, it's still not going to
            surprise you. The scenario is predictable due to the frequency that you have to consider
            what happened. Is it a big deal that you are spilling? No. You learned what you needed to learn, and
            either learn to stop spilling the milk, or accept the fate of your new shakey hands.
            At the end of the day it's just spilled milk.
          </li>
          <li>
            Now consider that you crash your car once a month.
            By atleast the third crash, youre going to start questioning what the hell is going on.
            You probably learn to specifically avoid whatever circumstance that caused the first two crashes.
            After you figure out the problem, you buy some new glasses and the crashes stop.
          </li>
        </ul>
      </div>
    </div>
    <p>The aspect of a circumstance that affects its td error isn't the reward magnitude,
      but the predictability. By that i mean, crashing being <strong>a big deal</strong>
      doesn't make the you more able to learn from it. This applies for the agent as well.
      The fact that spilling milk has a small effect on reward does not make it an
      easier circumstance for the agent to understand. What determines how easy the scenario is to understand
      is how often it happens. </p>
    <h5>The Problem</h5>
    <p>The consequence for not understanding milk spills is small.
      Wheras, the consequence for not understanding a serious but infrequent scenario, like car crashes, is great.
      So we would hope infrequent but impactful scenarios would be considered more by the agent.
      The problem is, in its current form, the agent does not do this. It considers scenarios only as often as they
      occur in the environment. Thats because the memory has
      the same proportion of "milk spillings" to "car crashes" as the environment. Shuffling doesn't change that ratio,
      and a batch of memories will probably have the same ratio as the entire memory, and as such, the same ratio
      as the environment.<br><br>
      You might protest.<br>
      <i>"The agent is dedicating learning steps proportionally to how often something occurs. Is it unreasonable
        to expect that to be an efficient use of time and processing?"</i><br><br>
      It is a somewhat reasonable way of thinking. More common scenarios would be useful to understand, because any
      trick you have
      for dealing with those scenarios will get lots of use. The problem is having "car-crash" scenarios ignored because
      they are buried under a million "milk-spills".
      It is pretty bad for the agent's performance.
    </p>
    <h4 class="u-align-center">A More Practical Example</h4>
    <p>It isnt really obvious what a "milk-spill" or a "car-crash" is in cartpole anyways, so let's consider a different
      game instead.
      Let's consider the agent is playing the original Super Mario Bros.
      If you are wondering why this game it's because I recently discovered our agent can never be good at mario.
      And the reason reveals there are probably many other environments with the same issue.
    </p>
    <h5>Fuck Pipes</h5>
    <p>In the very beginning of the very first level of mario there are three pipes.
      Each pipe is taller than the previous.</p>
    <div class="grid-container halves u-align-left">
      <img src="/tutorials/rl/prioritizedexperiencereplay/pipes.jpg" width=100%>
      <img src="/tutorials/rl/prioritizedexperiencereplay/pipes2.jpg" width=100%>
    </div>
    <p>The third pipe is quite tall, and the agent gets stuck there running and bouncing into the pipe until the time
      runs out and it dies. The agent will do this 3 times, and get a gameover. I ran the agent with many different
      settings, and let it train for 12-36 hours per trial. Out of the tens of thousands of attempts, only fifty or so
      runs made it over the pipe. Of those times, usually the agent got stuck at another pipe up ahead in exactly the
      same way.
      A handful of runs did actually beat the level, but this is pathetic.<br> I demand only excellence from my AI
      slaves.</p>
    <h5>The Chad Pipe</h5>
    <p>There are multiple reasons why these tall pipes are such a big challenge for the agent.
      But to understand those reasons, you need to know how hard it is to jump over one of these pipes.
      In mario not all jumps are the same height. The player can choose how high the jump goes by holding the
      jump button down for longer. This works up to a cap, which is the maximum jump height.
      A single press of the jump button causes mario to make a tiny little hop, which isn't anywhere near high
      enough to jump even the shorter pipes.
      To jump over one of the tall pipes, the player has to hold the jump button down for a while.
      It doesn't seem that long if you play the game in real time, but for the agent it is basically an eternity.
      I used the TAS (Tool Assisted Speedrun) Emulator, BizHawk, to investigate exactly how many consecutive frames the
      agent has to press the a button to jump that pipe.</p>
    <h3 class="u-align-center">It's 20 Frames</h3>
    <p>This agent was only able to jump the pipe a handfull of times due to the sheer unlikelyhood that it
      would press the a button for 20 frames straight. How unlikely is it? Let's consider the ways the buttons on
      the NES can be encoded. The controller has 8 buttons on it, 4 directions, start and select, a and b.
      So you might think thats 8 different possible actions, and only 6 if you remove start and select.
      If only things were so simple. Many maneuvers in games require pressing
      more than one button at a time, and as you know, our Deep Q Agent selects only one action per observation.
      So to give the agent access to every combination of simultanious button press available on the NES, it
      comes out to 256 different actions. (A + right, A + left, A + down, B + select, etc...). Obviously that's too
      many,
      and the agent will have a hell of a time exploring the results of 256 different actions, so it is common to use a
      reduced action space of about 12 actions involving only the most common button combinations (directions and a or
      b, and
      directions with a + b combined, and maybe a no-action, action). Jumping the pipe requires
      20 "a" frames in a row, and the last couple frames the agent needs
      to press right also to move over the top of the pipe. Well, really early on the agent learns to basically never
      let go of the right button.
      That leaves just the "a" presses to worry about. Consider the reduced 12 action case with 12 unique available
      button combos.
      To calculate the odds of 20 consecutive a presses in this action space, take the chance of pressing "a" multiplied
      by itself 20 times. So 1/12 * 1/12 * 1/12... or <code>(1/12)^20</code>. It comes out to 2.6*10^-22.
      My calculator couldn't even output this number because
      it is so small. It isn't as unlikely as picking-a-random-atom-in-the-universe-looking-for-a-specific-one rare, but
      it's still incredibly unlikely.
      Its like reaching into a tiny vial of hydrogen, randomly extracting an atom, and doing this repeatedly, hoping to
      find the specific one you want.<br><br>
      You might think im overstating the bad odds here.<br>
      <i>"Firstly, multiple actions in the 12-combo-actions space include an a press. So 1/12 should be more like 1/6.
        Secondly, surely the agent learned to press "a" and right a lot to
        make it to the pipe in the first place. The agent should be much more likely to press the buttons it needs to
        jump the pipe.
        Saying each action has a 1/12 or 1/6 chance of being pressed is misleading. Those chances obviously aren't equal
        to eachother."</i><br><br>
      While the agent does have a strong bias towards pressing right and 'a', you have to remember the agent doesn't
      pick
      its greedy action every time. Epsilon-greedy is forcing the agent to periodically choose a random action.
      So while the odds arent 2.6*10^-22 to 1, or <code>(1/6)^20 = 2.6e-16 to 1</code> (the odds given purely random
      button pressing),
      the odds are still incredibly bad.
    </p>
    <h5>A Good Car Crash</h5>
    <p>Basically, jumping over a pipe is an extreme case of a "car-crash" scenario. It's an event that, when it happens,
      has a gigantic impact on reward, and comes with a huge surprise. But it happens so infrequently that it is
      difficult, if not impossible, to learn from. The agent had no issue learning to jump over
      the first two pipes. If the third pipe was just a little bit shorter, then
      the agent would learn to jump over it because the scenario would happen more frequently.
      99.99999999%+ of learn steps will be batches of transitions that don't involve successfully jumping the pipe.
      Sadly, no amount of training will make the agent get good at jumping the pipe. The circumstance is just too rare.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Solution?</h4>
    <p>To solve part of this problem, the agent needs to set asside rare but impactful scenarios, and keep training on
      them.
      That doesn't fix the low likelyhood of accidentally jumping over the pipe. But, hopefully it ensures the agent
      only has to do it <strong>once</strong> or twice by accident before learning how to do it on purpose.
    </p>
    <h5>Implementation Specifics</h5>
    <p>What is the correct way to implent this?<br>
      You could make a seperate list of "car-crash" transitions, and then force the agent to run an additional
      learn step on that list each environment step. That way when a car-crash happens, it can't go unnoticed.
      But then you are burdened with determining which transitions
      deserve to be put in the list, and which don't. You could create a reward threshold hyperparameter that functions
      as the cutoff.
      Any transitions that have a reward magnitude above the threshold get thrown into the FIRE-DRILL list.
      That reward threshold would need to be tuned to each environment because each environment
      has different scales of rewards. Not only that but some sections of game environments are more
      sensetive than others, and the local reward deviations could be more subtle.<br><br>
      That sucks, and I can't be bothered to tune an endlessly growing pile of hyperparameters
      as our agent expands in complexity over time. Im a lazy bastard. So to avoid that manual work, the cutoff
      could be set so that any transition with a plus or minus reward deviation 50% greater than
      the average sample reward deviation goes into the FIRE-DRILL list. That average could even
      be a running window average, so it can pick up local trends in reward.
      But then you've still got to tune that percentage hyperparameter, and the running average window size...
      Plus, what if there is a car-crash transition that was at 149% the average reward, or 51%.
      It wont make it into the FIRE-DRILLS so it will go unnoticed.<br><br>
      What if there are "micro-car-crash" scenarios, samples that
      are more impactful and rare than the average, but not quite as world ending.
      You could set up a tiered system of 10 different FIRE-DRILL lists. Each one with a slightly larger
      cutoff from the last... and multiple sizes of running windows to focus the agent on different ranges
      of rareness and magnitude. How many tiers is enough?? How many windows?? What happepns when the
      agent has reviewed a specific car-crash enough times?? Does it still need to practice that one anymore? Do we
      need a remove cutoff to remove old ones as well?? Why does this sound like designing a whole new neural network
      architecture?
      Are we creating an animal operating system?
    </p>
    <h5>Woah. Calm Down</h5>
    <p>This is getting pretty complicated... The solution needs to be simple.
      It could take days to code what's described above, and the bugs could be crazy. Not to mention setting those
      hyperparameters would deserve its own research paper.
      What we need is for each transition to be on a continous spectrum of FIRE-DRILLness. Instead of sharp cutoffs,
      and seperate lists, use the list the agent already has.
    </p>
    <h5>Prioritized Memory Sampling</h5>
    <p>Currently when the agent has a learn() step, it samples the experience replay buffer to create a batch
      of transitions. All the samples have the same probability of being selected for
      a batch, <code>1 / memory_size</code>. This is the reason why the batch has the same ratio of car-crashes to
      milk-spills as the environment. The probability of seeing the car-crash is proportional to how rare it is.
      We need to rig the odds so that car-crash samples have a higher probability of being selected, and therefore
      learned from.
      Instead of sampling the memory with a flat probability, we could bias the probability of a memory getting chosen
      for
      a batch. The amount of bias could be determined by anything, but in our case we want to scale the bias
      by how surprised the agent was, the magnitude of the temporal difference error.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Time for Code</h4>
      <h5>Step One</h5>
      <p>First we need to add the ability to bias the sampling chances of our transitions so they 
        aren't all the same.
        The experience replay buffer is responsible for the sampling at the moment. Just add another array
        to the memory to store probabilities of being sampled.
      </p>
      <pre><code class="language-python">class ReplayBuffer():
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0

        self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64  )
        self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
        self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool   )
        self.priorityMemory     = np.zeros( self.memSize,               dtype=np.float32) # LOOK HERE

    def storeMemory(self, state, action, reward, nextState, done):
        memIndex = self.memCount % self.memSize 
        
        self.stateMemory[memIndex]      = state
        self.actionMemory[memIndex]     = action
        self.rewardMemory[memIndex]     = reward
        self.nextStateMemory[memIndex]  = nextState
        self.doneMemory[memIndex]       = done
        self.priorityMemory[memIndex]   = 100.0       # AND LOOK HERE

        self.memCount += 1</code></pre>
      <p>I called it priority, instead of probabilities, because the numbers don't really have to be between 0 and 1, which is what they
        should be if it was really a list of probabilities. Also, you might wonder why the priority of
        a new transition starts at 100. I just picked a nice big number. The priority needs to start high.
        If it starts low, then a brand new transition probably won't get chosen for a batch even once.
        That sample might as well not have existed. The initial priority should probably be chosen more smartly,
        but for now that's a distraction.
      </p>
      <h5>Step Two</h5>
      <p>Now that the memory can store sampling biases, the memory sample function needs to actualy use them.</p>
      <pre><code class="language-python">def sample(self, sampleSize):
    memMax = min(self.memCount, self.memSize)
    # batchIndices = np.random.choice(memMax, sampleSize, replace=False) # the old code

    priorities = torch.tensor(self.priorityMemory, dtype=torch.float32) # fetch priorities
    priorityDist = torch.distributions.Categorical(priorities[:memMax]) # fill a bag with colored marbles
    batchIndices = priorityDist.sample(sample_shape=(sampleSize,))      # pick marbles from the bag

    states      = self.stateMemory[batchIndices]
    actions     = self.actionMemory[batchIndices]
    rewards     = self.rewardMemory[batchIndices]
    nextStates  = self.nextStateMemory[batchIndices]
    dones       = self.doneMemory[batchIndices]

    return states, actions, rewards, nextStates, dones, batchIndices # returning the indices is NEW</code></pre>
    <p>Which transitions are selected is no longer flat random. Now it's biased random. 
      You've seen categorical distributions before in the beginning of the <a href="/tutorials/rl/actorcritic/actorcritic.html">actor-critic tutorial</a>.
    But, basically we put colored marbles into a bag. The number of marbles each transition gets is its priority. 
    Ex: If a transition has a priority of 100 it means it gets 100 marbles of its index in the bag. 
    Then you draw marble until you have enough to fill your batch. There can be repeats. It's totally fine. 
    Also the function returns the indices now. You'll see why in a sec.</p>
    <h5>Step Three</h5>
    <p>The agents learn function gets the batch from the memory, so the prioritized sampling should already be
      in effect. However, the priorities are going to stay at 100... That's no good. 
      The priority of a memory should be how surprising it was to the agent, the td error. 
      The td error of a transition can then be stored back as the transitions new priority.
    </p>
    <pre><code class="language-python">def learn(self):
    if self.memory.memCount < self.minMemorySize:
        return

    # the sample indices are fetched too now |-right here-|
    states, actions, rewards, states_, dones, memIndices = self.memory.sample(self.batchSize)
    states  = torch.tensor(states , dtype=torch.float32).to(self.network.device)
    actions = torch.tensor(actions, dtype=torch.long   ).to(self.network.device)
    rewards = torch.tensor(rewards, dtype=torch.float32).to(self.network.device)
    states_ = torch.tensor(states_, dtype=torch.float32).to(self.network.device)
    dones   = torch.tensor(dones  , dtype=torch.bool   ).to(self.network.device)

    batchIndices = np.arange(self.batchSize, dtype=np.int64)
    qValue = self.network(states)[batchIndices, actions]

    qValues_ = self.network(states_)
    qValue_ = torch.max(qValues_, dim=1)[0]
    qValue_[dones] = 0.0

    qTarget = reward + self.gamma * qValue_
    loss = self.network.loss(qTarget, qValue)

    '''PRIORITIES ARE UPDATED HERE'''
    td = qTarget - qValue   #   temporal difference, (almost same as loss)
    self.memory.td[memIndices] = td.detach().abs() #   update priorities

    self.network.optimizer.zero_grad()
    loss.backward()
    self.network.optimizer.step()

    self.epsilon -= self.epsilon_decay
    if self.epsilon < self.epsilon_min:
        self.epsilon = self.epsilon_min</code></pre>
    <p>Now you can see why the batch indices needed to be passed through. They are needed to assign new priorities 
      into the replay buffer.
    </p>
    <pre><code class="language-python">td = qTarget - qValue   #   temporal difference, (almost same as loss)
self.memory.td[memIndices] = td.detach().abs() #   update priorities</code></pre>
    <p>You're probably wondering why to recalculate td, since the loss function is already doing this subtraction 
      internally. Honestly it's just easy to do it like this. I doubt doing 64 subtractions is 
      going to incur any noticeable performance drop. And it's way less work than going and re-engineering 
      the loss function ourselves just to reuse the subtraction.<br><br>
      Also you can see that we aren't using the raw td, we are taking the absolute value of it. 
      Some surprises are car-crashes, and others are lottery-wins. The magnitude of the surprise is what is important in 
      determining whether a transition should be reviewed again. That's not to say you couldn't bias the agent 
      to be more optimistically focused than pessimistic, but at the moment the agent is a fairly reasonable kind of guy. 
      The lady agents don't find it very romantic.
    </p>
    <h4 class="u-align-center">Time for Testing</h4>
    <p>We only changed 10 lines of code. That's great. But does it work? 
      To test something like this we're gonna have to do multiple runs of the old code vs multiple runs of the new code. 
      Ill graph 10 runs from each, where each run goes until it gets 50 episodes in a row that last for 200 
      steps without dropping the pole.
    </p>
    <img src="/tutorials/rl/prioritizedexperiencereplay/noexponent.jpg" width=100%>
    <p>The image is very compressed for phone users, but you can see that the blue agent got higher scores earlier 
      than the orange agent. Wow, that's great. Except theres just one problem. Blue is the agent without prioritized 
      experience replay. Orange is the new code.<br><br>
      So PER sucks? No, it doesn't suck. It's great, as demonstrated in the original <a href="https://arxiv.org/abs/1511.05952">paper</a>.
      Our PER sucks. Let's investigate those priorities.  
    </p>
    <p>I put this code in the main loop.</p>
    <pre><code class="language-python">if score > 100:
    memMax = min(agent.memory.memCount, agent.memory.memSize)
    print(agent.memory.priorityMemory[:memMax])
    print(agent.memory.priorityMemory[:memMax].max())
    print(agent.memory.priorityMemory[:memMax].mean())</code></pre>
    <p>And here are the prints...</p>
    <pre><code class="language-python">...
9.81832504e-01 8.61540794e-01 1.13772488e+00 9.08970833e-01
9.69904900e-01 7.01960564e-01 6.44301414e-01 4.78118896e-01
5.88344574e-01 2.69694328e-01 3.67557526e-01 6.77224159e-01
3.56877327e-01 5.16098022e-01 4.01191711e-02 4.76974487e-01
4.45224762e-01 5.12881279e-01 3.70181084e-01 1.23029709e-01
5.47778130e-01 3.72940540e-01 7.13039875e-01 7.90067673e-01
3.81707668e-01 5.62210083e-01 8.12056065e-01 8.49058628e-01
9.64502811e-01 3.74630451e-01 7.71329880e-01 7.46834755e-01
8.80380154e-01 9.21054363e-01 8.86570930e-01 1.01419759e+00
9.17692423e-01 8.95126343e-01 7.80601740e-01 1.07545328e+00
8.76361132e-01 1.26037169e+00 9.33368206e-01 1.00000000e+02
1.20824647e+00 1.01140404e+00 1.05878687e+00 1.00000000e+02
1.13288975e+00 1.12992764e+00 1.00000000e+02 1.00000000e+02
1.00000000e+02 1.26903963e+00 8.25684071e-01 1.25932932e+00
8.55631828e-01 8.39446068e-01 1.00000000e+02 1.00000000e+02
1.07417965e+00 8.56302261e-01 4.51126575e-01 1.09786844e+00
1.09506273e+00 7.49367714e-01 4.81671810e-01 8.34473610e-01
9.24391508e-01 1.00000000e+02 6.76158905e-01 1.00000000e+02
9.44183111e-01 1.00000000e+02 1.00000000e+02 1.00000000e+02
1.00000000e+02 1.00000000e+02 1.00000000e+02 1.00000000e+02
1.00000000e+02]
100.0 # must be a max, looks like an new memory (since its 100)
2.460974  # mean value</code></pre>
<p>There are an awful lot of 100's in there too, 
  which doesn't seem right. However, considering that the print is the tail end of the memory, that means 
  out of maybe ten thousand samples, theres just a few 100s in there, and they are brand new. 
  So thats not so bad. It looks like it takes about 9 steps for new samples to get processed 
  the first time. Is this the birth of a reaction time?<br><br> Why is the mean value almost 2.5
  when most of the numbers printed out are less than 1.0? Those numbers vary by a lot, which is to be expected, 
  but if the variation is too great, it could be a sign some transitions are dominating learn steps. 
  This could be a good thing.
  However, if the agent is left going for a while, after enough 500's the loss will suddenly spike 
  and the score will jump to 9, the minimum score. I think the problem is that the prioritization 
  is too strong.
</p>
<h5>An Assertion</h5>
<p>To test this idea we can introduce another hyperparamater. Yay.
This hyperparameter will control the degree of prioritization. More specifically, it 
will scale variation between sample td's to be wider, or thinner. 
Let's call this hyperparameter "surprise". Because, it scales how surprisable 
the agent is.</p>
<pre><code class="language-python">class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)
        self.memory = ReplayBuffer(maxSize=10000, stateShape=inputShape)
        self.minMemorySize = 64
        self.batchSize = 64
        self.gamma = 0.99
        self.surprise = 0.5 # here's the new stuff
...
        td = qTarget - qValue
        priority = td.abs() ** self.surprise  # squash the td error
        self.memory.priorityMemory[memIndices] = priority.detach().cpu()</code></pre>
<p>0.5 is just a middle number, but its possible the ideal value is 0.645345 or something.<br>
  Anyways, lets test it.
</p>
<img src="/tutorials/rl/prioritizedexperiencereplay/noexponent.jpg" width=100%>

      <p>I hope you can see how this solution addresses the problems with the messy solution proposed before.
        Using this continous spectrum of priority is like having an infinite number of tiers in a tiered FIRE-DRILL
        list.
        Micro-car-crash samples are not likely to be ignored. It works regardless of the scale of reward
        in an environment. And, as the agent trains on a car-crash sample, the td
        error will go down. This means the priority of that sample will go down, and so it slowly gets "removed" from
        the fire drills,
        making room for newer, or more subtle car-crash samples.
        It is a really great solution, and it wasn't my idea. It was debuted and heavily investigated in
        <a href="https://arxiv.org/abs/1511.05952">this paper</a>, which I encourage you to read.
      </p>

      <p>There are other opportunities for biasing the sampling probabilities of the memory.
        I came up with a fun one that comes as a bonus in this tutorial. ;)
      </p>
      <ul>There are many other causes for the pipe difficulty issue. We focused on one specific cause because
        it could be addressed with a relatively simple solution.
        However, the other causes have not been addressed. They still exist as serious flaws in the agent's reasoning.
        They could be fixed with different solutions, and probably run alongside Prioritized Experience Replay.
        Here are some other agent shortcomings that might also cause the Pipe Problem.
        <li>The agent cant remember what it's </li>
      </ul>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Code Review</h4>
    <p>This is what we are working with. There are some <code>HERE</code> comments marking what needs attention most.
    </p>
    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym       
import math
import numpy as np
import random

'''FINE: this class actually looks mostly fine as is.
Sure, the number of layers and layer dimensions aren't passed in as arguments 
to the constructor, but that's a lot of boilerplate I don't want to inflate the code with.
Yes, that is useful if you want to programmatically try different architectures, but 
it's not necessary generally.'''
class Network(torch.nn.Module):
    def __init__(self, alpha, inputShape, numActions):
        super().__init__()
        self.inputShape = inputShape
        self.numActions = numActions
        self.fc1Dims = 1024
        self.fc2Dims = 512

        self.fc1 = nn.Linear(*self.inputShape, self.fc1Dims)
        self.fc2 = nn.Linear(self.fc1Dims, self.fc2Dims)
        self.fc3 = nn.Linear(self.fc2Dims, numActions)

        self.optimizer = optim.Adam(self.parameters(), lr=alpha)
        self.loss = nn.MSELoss()

        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cpu")
        self.to(self.device)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)

    def chooseAction(self, observation):
        state = torch.tensor(observation).float().detach()
        state = state.to(self.network.device)
        state = state.unsqueeze(0)

        qValues = self.network(state)
        action = torch.argmax(qValues).item()

        '''HERE: this should probably be changed for a more sane exploration method.'''
        chanceOfAsparagus = random.randint(1, 10)
        if chanceOfAsparagus == 1:  #   10% chance
            action = random.randint(0, 1)

        return action

    def learn(self, memory, batchSize):
        '''HERE: theres no minimum memory size??'''
        if len(memory) < batchSize:
            return 

        self.network.optimizer.zero_grad()

        '''HERE: this entire block runs very slow compared to the common implementation'''
        randomMemories = random.choices(memory, k=batchSize)
        memories = np.stack(randomMemories)
        states, actions, rewards, states_, dones = memories.T
        states, actions, rewards, states_, dones = \
            np.stack(states), np.stack(actions), np.stack(rewards), np.stack(states_), np.stack(dones)
        
        states  =   torch.tensor( states    ).float().to(self.network.device)
        actions =   torch.tensor( actions   ).long().to(self.network.device)
        rewards =   torch.tensor( rewards   ).float().to(self.network.device)
        states_ =   torch.tensor( states_   ).float().to(self.network.device)
        dones   =   torch.tensor( dones     ).to(self.network.device)

        qValues = self.network(states)
        nextQValues = self.network(states_)

        batchIndecies = np.arange(batchSize, dtype=np.int64)

        '''HERE: the names of these variables don't match the common names, but 
        the functionality is all here, and the equation is fine. 
        Except it is missing one part, but that will be explained.'''
        nowValues = qValues[batchIndecies, actions]    #   interpret the past
        futureValues = torch.max(nextQValues, dim=1)[0]    #   interpret the future
        futureValues[dones] = 0.0   #   ignore future actions if there will 
                                    #   be no future actions anyways
        trueValuesOfNow = rewards + futureValues    #   same temporal difference
        loss = self.network.loss(trueValuesOfNow, nowValues)

        loss.backward()
        self.network.optimizer.step()

'''FINE: this "main agent loop" is actually really close to the common drl code.'''
if __name__ == '__main__':
    env = gym.make('CartPole-v1').unwrapped
    agent = Agent(lr=0.001, inputShape=(4,), numActions=2)
    BATCH_SIZE = 64
    '''HERE: the memory deserves an upgrade'''
    memory = []

    highScore = -math.inf
    episode = 0
    numSamples = 0
    while True:
        done = False
        state = env.reset()

        score, frame = 0, 1
        while not done:
            # env.render()

            action = agent.chooseAction(state)
            state_, reward, done, info = env.step(action)

            '''HERE: the memory will change so this will change a bit'''
            transition = [state, action, reward, state_, done]
            memory.append(transition)
            
            agent.learn(memory, BATCH_SIZE)
            state = state_

            numSamples += 1

            score += reward
            frame += 1

        highScore = max(highScore, score)

        print(( "total samples: {}, ep {}: high-score {:12.3f}, "
                "score {:12.3f}").format(
            numSamples, episode, highScore, score,frame))

        episode += 1</code></pre>
  </div>
  <div class="grid-container full u-align-left">
    <h3 class="u-align-center">Upgrades</h3>
    <p>The three aspects of this code most needing improvement are the experience replay buffer,
      the td code in the learn function, and the exploration strategy.
    </p>
    <h4 class="u-align-center">Exploration Strategy</h4>
    <p>At the moment the agent is picking a random action 10% of the time. This is to force
      the agent to try different actions. Regular exploration ensures enough data is collected
      to get the neural network to learn the environments reward function. Mainly, it pushes
      the agent into circumstances it wouldnt find itself in otherwise. Exploration is covered more deeply in the
      <a href="/tutorials/rl/deepqlearning/deepqlearning.html">DQLearning Tutorial</a> and the
      <a href="/tutorials/rl/actorcritic/actorcritic.html">Actor Critic Tutorial</a>.
      The primary issue with the code as is, is the exploration rate. It doesn't change.
      As the qvalues become more refined, the greedy action is often a better choice for the agent.
      If the exploration rate remains high, then the agent might not get to refine it's strategy.
      Also the code has the word asparagus in it.
    </p>
    <pre><code class="language-python">class Agent(): # NEW AND IMPROVED
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)

        # exploration parameters
        self.epsilon = 0.1            # chance of random action
        self.epsilon_decay = 0.00005  # how much the chance shrinks each step
        self.epsilon_min = 0.001      # minimum for the chance, so you never fully stop exploring

    def chooseAction(self, observation):
        if np.random.random() < self.epsilon: # generate a num between 0.0 and 1.0 to "roll"
            action = random.randint(0, 1)
        else: # dont bother doing all that torch stuff if you're just gonna choose a random
            state = torch.tensor(observation).float().detach()
            state = state.to(self.network.device)
            state = state.unsqueeze(0)

            qValues = self.network(state)
            action = torch.argmax(qValues).item()
        return action</code></pre>
    <p>The new improved code uses the <strong>epsilon-greedy</strong> exploration strategy. It works just like the old
      exploration strategy:</p>
    <pre><code class="language-python">chanceOfAsparagus = random.randint(1, 10)
if chanceOfAsparagus == 1:  #   10% chance
    action = random.randint(0, 1)</code></pre>
    <p>
      Except instead of a fixed 10% chance, the chance starts high and shrinks until it hits a minimum.
      Why is it called epsilon? More math history or something. It is generally denoted by the <code>ϵ</code> character,
      epsilon...
      In the mainstream it appeared around 1998, maybe in the Sutton and Barto book on reinforcement learning.
      Probably before that.
      Anyways, epsilon is a number between 0 and 1, that should
      start high, and slowly become smaller. Generate a random number between zero and one, and then compare it to
      the exploration threshold, <strong>epsilon</strong>. If epsilon is 0.9, then 90% of the time the random number
      will be smaller than epsilon, so 90% of the time the action will be random.

      <strong>Epsilon-greedy</strong> is probably the most common exploration strategy.
      More generally though, theres usually some way actions are randomized.
      You will see epsilon-greedy all over reinforcement learning.
    </p>
    <h5>Common Concerns</h5>
    <p>Dont forget to actually shrink epsilon, and cap it at the minimum value.
      You can do it periodically but I most commonly see the decay done once per learn step at the bottom of
      the learn function.
    </p>
    <pre><code class="language-python">def learn(self, memory, batchSize):
    ... # bla bla bla the rest of the learn function above
    trueValuesOfNow = rewards + futureValues    #   same temporal difference
    loss = self.network.loss(trueValuesOfNow, nowValues)

    loss.backward()
    self.network.optimizer.step()

    '''SHRINK EPSILON HERE'''
    self.epsilon -= self.epsilon_decay  # shrink
    if self.epsilon < self.epsilon_min: # clamp
        self.epsilon = self.epsilon_min</code></pre>
    <p>
      <div class="grid-container halves u-align-left">
        <div class="left">
          <h5>Greater Than Less Than</h5>
          <p>Make sure to get the direction of the <code>&lt</code> vs <code>&gt</code> correct when comparing to
            epsilon to pick a random action. If you get it backwards, your exploration chance is inverted.</p>
          <pre><code class="language-python"># epsilon = 0.1

# this is 10% chance
if np.random.random() < self.epsilon:   
    action = random.randint(0, 1)

# this is 90% chance
if np.random.random() > self.epsilon:   
    action = random.randint(0, 1)</code></pre>
          <p>And worse, if it is backwards, your exploration chance will actually increase instead of decrease when you
            shrink epsilon. As you can imagine, this results in terrible scores, as the agent will slowly
            take more and more random moves.
          </p>
          <h5>When In Doubt, Print</h5>
          <p>If the agent is being dumb, print out epsilon at each step, or episode, to make sure
            it is doing what you want.
          </p>
        </div>
        <div class="right">
          <h5>How To Set Settings</h5>
          <p>That epsilon_decay number I picked seems kinda random doesn't it?</p>
          <pre><code class="language-python">self.epsilon = 0.1    
self.epsilon_decay = 0.00005  
self.epsilon_min = 0.001</code></pre>
          <p>If you set these wrong the agent won't learn at all. You might
            think it's an issue with any other part of your code, which could send you on an hours/days bug hunt.
            Luckily, these new hyperparameters can be made to be equivalent to the old code to give a starting point
            that
            you can be confident will work.
          </p>
          <pre><code class="language-python">self.epsilon = 0.1    
self.epsilon_decay = 0.0
self.epsilon_min = 0.1</code></pre>
          <p>If epsilon is 0.1, and the decay rate is 0.0, then it is a constant flat 10% chance of random action.
            This should yield exactly the same results as the old code. (It does. I tested it.)
            So you can start from this, and slowly raise the decay and shrink the min from there.
            <code>self.epsilon_decay =
              0.00000001</code> That might be a bit small of a decay rate, but maybe you can aim epsilon so that
            by a certain episode it hits a target minimum. The right epsilon and decay rate heavily depends on the
            environment, though.
            One of the benefits of using this standard epsilon-greedy implementation is you can now use other peoples
            espilon-greedy settings. Yay.
          </p>
        </div>
      </div>
      <h5>The Future of Exploration</h5>
      Epsilon-greedy is not perfect.
      Notice epsilon can only decrease, and never increase. This could be considered a feature,
      but also a flaw.
      It makes assumptions about how the best strategy is developed, and about how unchanging the environment is.
      When a human finds that it is not getting good results, it actively tries new strategies (espilon goes up).
      If a human finds its current strategy to be paying off, it won't try new things (epsilon goes down).
      Real animals use adaptive exploration rates.
      So, maybe the agent's exploration rate should increase if reward is unexpectedly low.
      Maybe it should change if the reward is staying the same for a while.
      Perhaps it should decrease based on queues from the environment, and should be a function learned across
      environments with a neural network.
      It's a whole other upgrade just waiting to be discovered.<br>
      Another issue with this exploration strategy is that it explores by randomizing action,
      as opposed to randomizing the goal. Humans practice tasks often by learning similar modified versions of the task.
      Even if somebody doesn't know what optimal chess play looks like, they still know they will get better at chess by
      playing alternate versions, and chess minigames. Sometimes these minigames have
      different goals than normal chess, but that doesn't make them misaligned with learning some of the same skills one
      would
      use in regular chess. For reinforcement learning this is also the case.<br>
      For a mechanical task, I was thinking along the lines of practicing
      shooting an arrow at a target, by training to miss the target by a certain amount on purpose. A big advantage of
      this feature is that it allows for training the agent on arbitrary goals, preventing overfitting to the main goal
      of the environment.
      Which would you rather have, an agent that can hit any target with an arrow? Or an agent that can only hit one?
      This goal exploration effect could be achieved by micromanaging which environments the agent plays,
      but it would be much more valuable to build it into the agent as an exploration strategy. The agent would
      sometimes try different goals, and ignore the environmental reward. I think I saw a paper along these lines where
      the goal is augmented to give the agent a better sense of the real primary goal.
      Another upgrade worth investigating.
    </p>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Numpy Experience Replay</h4>
    <p>Maybe you noticed before, but the old code gets slower over time.
      This is partially due to the agent getting better at the game. As the agent gets better, the episodes require
      more steps to end. However, you probably noticed if you let the game go for a few thousand episodes, the episodes
      start to get really slow, even though the scores are roughly the same. (For the record, a few thousand
      episodes is not that many. DRL papers often consider agents trained on millions of samples.)
      What is the cause of this slowdown?
      Nothing in our agent seems to require more processing at a later episode than an early episode.
      It should be a constant amount of processing per learn step... except the code involving the experience replay
      buffer.
      This isn't too much of a problem for "read", but the moment you might try to "write" back to the buffer inside
      your learn step, runtime performance suffers too much. It starts to get unbearably slow.<br>
      Additionally, your options for batch size are limited by the sampling speed of your replay buffer.
      As the replay buffer gets longer, sampling from it becomes slower. (relative to numpy arrays atleast)
      We mostly avoided that effect by being careful up to this point. But it's better to have a high power
      tool to play with.
      A lot of the agent upgrades involve sampling data from the memory,
      computing something, and sometimes even storing (regularly processing the memories, and storing information about
      them).
      Doing this with python arrays starts to become a performance problem,
      especially once there are some python for loops in there. ew :^)
      So think of this as an infrastructural investment. The numpy indexing tricks end up being incredibly convenient,
      and
      besides, it's the most common way to do memories anyways.
    </p>
    <p>So, this:</p>
    <pre><code class="language-python">memory = []</code></pre>
    <p>becomes:</p>
    <pre><code class="language-python">class ReplayBuffer(): '''NEW CODE'''
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0

        self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64)
        self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
        self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool)

    def storeMemory(self, state, action, reward, nextState, done):
        memIndex = self.memCount % self.memSize 
        
        self.stateMemory[memIndex]      = state
        self.actionMemory[memIndex]     = action
        self.rewardMemory[memIndex]     = reward
        self.nextStateMemory[memIndex]  = nextState
        self.doneMemory[memIndex]       = done

        self.memCount += 1

    def sample(self, sampleSize):
        memMax = min(self.memCount, self.memSize)
        batchIndecies = np.random.choice(memMax, sampleSize, replace=False)

        states      = self.stateMemory[batchIndecies]
        actions     = self.actionMemory[batchIndecies]
        rewards     = self.rewardMemory[batchIndecies]
        nextStates  = self.nextStateMemory[batchIndecies]
        dones       = self.doneMemory[batchIndecies]

        return states, actions, rewards, nextStates, dones</code></pre>
    <p>Notice how much code this adds. There's a reason i don't encourage
      people to start with this. I apologize for inflating your code now, but it's time.</p>
    <h5>Struct Of Arrays</h5>
    <p>Each piece of the transition is stored in its own array. You premake and the arrays to a certain size when
      you create the memory.
    </p>
    <pre><code class="language-python">self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64)
self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool)</code></pre>
    <p>Notice the names of these arrays correspond to the same SARS you are familiar with.</p>
    <div class="grid-container halves u-align-left">
      <div>
        <h5>Class Inputs</h5>
        <pre><code class="language-python">class ReplayBuffer(): '''NEW CODE'''
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0</code></pre>
        <p>
          The class takes in the max size of the buffer, and the shape of the states.<br>

          Ex: cartpole has a state shape of 4 numbers, so the stateShape
          should be <code>(4,)</code>. If you just pass in <code>stateShape=4</code> it wont work.
          That's because there is some <strong>tuple unpacking</strong> going on in the arrays.
          (See the <code>*stateShape</code> in the array allocation.)<br><br>
          In python <code>(4,)</code> and <code>(4)</code> are
          not the same thing. Sounds crazy right? It's a noob trap. Go ahead and try it in the
          terminal. <code>print((4))</code> vs. <code>print((4,))</code><br><br>
        </p>
        <h5>Sampling</h5>
        <p>Getting the transitions out isn't so difficult, and most importantly, it
          takes the same amount of time, regardless of how big the memory is.
        </p>
        <pre><code class="language-python">batchIndecies = np.random.choice(
    memMax, sampleSize, replace=False)

states      = self.stateMemory[batchIndecies]
actions     = self.actionMemory[batchIndecies]
...</code></pre>
        <p>First pick some random indices, and then just use the numpy indexing magic
          to get all the right elements out at once.
        </p>
      </div>
      <div>
        <h5>Indexing Complications</h5>
        <p>You may have noticed a few weird lines.</p>
        <pre><code class="language-python">self.memCount = 0
# and 
memIndex = self.memCount % self.memSize
# and
memMax = min(self.memCount, self.memSize)</code></pre>
        <p>The way this replay buffer works is the arrays are given a length before hand.
          So we have to keep track of how many memories were currently stored to know where to
          put the new transitions. <code>self.memCount += 1</code></p>
        <p>What happens if you store memories after the buffer is full?</p>
        <pre><code class="language-python"># the index rolls over back to the begining.
memIndex = self.memCount % self.memSize 

#  this overwrites the oldest memory
self.stateMemory[memIndex]      = state
self.actionMemory[memIndex]     = action
self.rewardMemory[memIndex]     = reward
self.nextStateMemory[memIndex]  = nextState
self.doneMemory[memIndex]       = done</code></pre>
        <p>Before the memory has been filled all the way, you have to avoid sampling from parts of the arrays
          that haven't been assigned data to yet. They will just hold garbage.
        </p>
        <pre><code class="language-python">#  that should explain this line in sample()
memMax = min(self.memCount, self.memSize)</code></pre>
      </div>
    </div>
  </div>
  <div class="grid-container full u-align-left">
    <h4>Now To Use It</h4>
    <p>To use the new replay buffer we have to do some refactoring.
      First, remove it from the main loop entirely, and put the replay buffer into the agent.
      This isnt the only way to do it, but I feel like doing it this way. I'm writing the
      tutorial, so I am your god now. You have to do what I say. Give me your money.</p>
    <pre><code class="language-python">class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)
        self.memory = ReplayBuffer(maxSize=100000, stateShape=inputShape)
        self.batchSize = 64</code></pre>
    <p>Also we need to wrap the memory store function so you can call it from the agent.</p>
    <!-- <pre><code class="language-python"></code></pre> -->
    <pre><code class="language-python">#    this function is in the agent class
def storeMemory(self, state, action, reward, nextState, done):
    self.memory.storeMemory(state, action, reward, nextState, done)</code></pre>
    <p>Inside the main function its a little bit cleaner now.</p>
    <pre><code class="language-python">...
score, frame = 0, 1
while not done:
    # env.render()

    action = agent.chooseAction(state)
    state_, reward, done, info = env.step(action)
    agent.storeMemory(state, action, reward, state_, done)  #   use the wrapped memory store function
    agent.learn()   #   no more arguments go into learn()
                    #   the agent has everything it needs
    state = state_
...</code></pre>
    <h5>Learn Function Adjustments</h5>
    <p>So that's all setup... now to fix the learn function.</p>
    <pre><code class="language-python">def learn(self, memory, batchSize):
    if self.memory.memCount < self.batchSize:   #   this changed, but does the same thing
        return

    ''' this stuff gets replaced with the ReplayBuffer sample function, 
        which basically does the same thing internally. 
        just without the stacking and python choice'''
    # randomMemories = random.choices(memory, k=batchSize)
    # memories = np.stack(randomMemories)
    # states, actions, rewards, states_, dones = memories.T
    # states, actions, rewards, states_, dones = \
    #     np.stack(states), np.stack(actions), np.stack(rewards), np.stack(states_), np.stack(dones)
    states, actions, rewards, states_, dones = self.memory.sample(self.batchSize)

    #   still need to pass the stuff to the gpu
    states  = torch.tensor(states , dtype=torch.float32).to(self.network.device)
    actions = torch.tensor(actions, dtype=torch.long   ).to(self.network.device)
    rewards = torch.tensor(rewards, dtype=torch.float32).to(self.network.device)
    states_ = torch.tensor(states_, dtype=torch.float32).to(self.network.device)
    dones   = torch.tensor(dones  , dtype=torch.bool   ).to(self.network.device)
    #   pep8 python style guidelines and google don't want you to format things like this.
    #   but i'm god, and this is my tutorial dimension. Mr.Google has no power here
...</code></pre>
    <p>The rest of the learn function code is compatable with the new memory. Almost...
      There are a couple of lines that need changing. And, while we are at it let's
      try some more standard naming conventions.</p>
    <pre><code class="language-python">'''OLD CODE'''
qValues = self.network(states)
nextQValues = self.network(states_)

batchIndecies = np.arange(self.batchSize, dtype=np.int64)

nowValues = qValues[batchIndecies, actions]    #   interpret the past
futureValues = torch.max(nextQValues, dim=1)[0]    #   interpret the future
futureValues[dones] = 0.0   #   ignore future actions if there will 
                            #   be no future actions anyways
trueValuesOfNow = rewards + futureValues    #   same temporal difference
loss = self.network.loss(trueValuesOfNow, nowValues)

'''NEW CODE'''
batchIndices = np.arange(self.batchSize, dtype=np.int64)  # i learned how to spell indices
qValue = self.network(states)[batchIndices, actions]

qValues_ = self.network(states_)        #   values of all actions
qValue_ = torch.max(qValues_, dim=1)[0] #   extract greedy action value
qValue_[dones] = 0.0                    #   filter out post-terminal states

td = reward + self.gamma * qValue_      #   temporal difference
loss = self.network.loss(td, qValue)
...</code></pre>
    <p>It's a little more compact, but more importantly other people who look
      at your code will recognize the idioms.<br>
      Also, you may have noticed <strong>gamma</strong> is back, which you might remember from the
      <a href="/tutorials/rl/actorcritic/actorcritic.html#improving-our-choices-in-life">actor-critic tutorial</a>.
      I didn't explain it too much then, but basically its a bias towards the present.
      Without it you might get circumstances where the agent could wait indefinitely for some anticipated reward.
      Realistically, that's never going to happen, but hey now you have another hyperparamater to worry about.
      Lucky you.
      A gamma value of 0.99 is fine. I've never had to
      change it. But, in certain environments a lower gamma improves performance. By the way, don't forget to put gamma
      into the agent class definition.
    </p>
    <pre><code class="language-python">class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)
        self.memory = ReplayBuffer(maxSize=100000, stateShape=inputShape)
        self.batchSize = 64
        self.gamma = 0.99   #   did i really need to show you this?</code></pre>
    <h5>Learned Gamma</h5>
    <p>This is another case where a hyperparameter imposes assumptions about the nature of the environment.
      Gamma should probably vary during the agent's life. Maybe learned with a neural network across
      environments or just tuned based on td error and state familiarity. It is yet another upgrade idea that
      facilitates a new type of adaptation.
    </p>
    <h4 class="u-align-center">Minimum Memory Fullness</h4>
    <p>Something that you will see if you peruse the openAI baselines is a minimum replay buffer fullness.
      They seem to set it to 20,000 samples for some reason. The agent won't learn from any samples, until
      the memory hits the minimum threshold. It is bad to sample from a barely filled replay buffer.
      An agent that does that will end up seeing the first few memories
      tons of times, and so they will be overrepresented in the neural network. Also the buffer isn't very
      diverse at first, so there is even more risk for overfitting. Setting a minimum memory size reduces both of these
      issues.
      Though, it does mean the agent will not be doing any learning for the earliest portion of episodes, so
      don't expect to see any good performance while the buffer is still filling up.
      In my personal experience, I have found that adding a minimum buffer fullness
      really stabilizes agent performance. There are less weird sudden drops in score.
      My guess is that without a minimum memory the network learns a bunch of noise that
      it has to unlearn later on. Play with it to see if you can replicate that issue.
      (Try setting the minimum to a low number like the batch size, then up to a higher number like 2048.)
    </p>
    <pre><code class="language-python">class Agent():
def __init__(self, lr, inputShape, numActions):
    self.network = Network(lr, inputShape, numActions)
    self.memory = ReplayBuffer(maxSize=10000, stateShape=inputShape)
    self.minMemorySize = 1024
...
    def learn(self):
        if self.memory.memCount < self.minMemorySize:   #   this replaced self.batchSize
            return
...</code></pre>
  </div>
  <div class="grid-container full u-align-left">
    <h4 class="u-align-center">Full Code</h4>
    <p>Welcome your code to the 22nd century.</p>
    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym       
import math
import numpy as np
import random

class ReplayBuffer():
    def __init__(self, maxSize, stateShape):
        self.memSize = maxSize
        self.memCount = 0

        self.stateMemory        = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.actionMemory       = np.zeros( self.memSize,               dtype=np.int64  )
        self.rewardMemory       = np.zeros( self.memSize,               dtype=np.float32)
        self.nextStateMemory    = np.zeros((self.memSize, *stateShape), dtype=np.float32)
        self.doneMemory         = np.zeros( self.memSize,               dtype=np.bool   )

    def storeMemory(self, state, action, reward, nextState, done):
        memIndex = self.memCount % self.memSize 
        
        self.stateMemory[memIndex]      = state
        self.actionMemory[memIndex]     = action
        self.rewardMemory[memIndex]     = reward
        self.nextStateMemory[memIndex]  = nextState
        self.doneMemory[memIndex]       = done

        self.memCount += 1

    def sample(self, sampleSize):
        memMax = min(self.memCount, self.memSize)
        batchIndecies = np.random.choice(memMax, sampleSize, replace=False)

        states      = self.stateMemory[batchIndecies]
        actions     = self.actionMemory[batchIndecies]
        rewards     = self.rewardMemory[batchIndecies]
        nextStates  = self.nextStateMemory[batchIndecies]
        dones       = self.doneMemory[batchIndecies]

        return states, actions, rewards, nextStates, dones

class Network(torch.nn.Module):
    def __init__(self, alpha, inputShape, numActions):
        super().__init__()
        self.inputShape = inputShape
        self.numActions = numActions
        self.fc1Dims = 1024
        self.fc2Dims = 512

        self.fc1 = nn.Linear(*self.inputShape, self.fc1Dims)
        self.fc2 = nn.Linear(self.fc1Dims, self.fc2Dims)
        self.fc3 = nn.Linear(self.fc2Dims, numActions)

        self.optimizer = optim.Adam(self.parameters(), lr=alpha)
        self.loss = nn.MSELoss()
        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cpu")
        self.to(self.device)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

class Agent():
    def __init__(self, lr, inputShape, numActions):
        self.network = Network(lr, inputShape, numActions)
        self.memory = ReplayBuffer(maxSize=10000, stateShape=inputShape)
        self.minMemorySize = 1024
        self.batchSize = 64
        self.gamma = 0.99

        self.epsilon = 0.1
        self.epsilon_decay = 0.00005
        self.epsilon_min = 0.001

    def chooseAction(self, observation):
        if np.random.random() < self.epsilon:
            action = random.randint(0, 1)
        else:
            state = torch.tensor(observation).float().detach()
            state = state.to(self.network.device)
            state = state.unsqueeze(0)

            qValues = self.network(state)
            action = torch.argmax(qValues).item()
        return action

    def storeMemory(self, state, action, reward, nextState, done):
        self.memory.storeMemory(state, action, reward, nextState, done)

    def learn(self):
        if self.memory.memCount < self.minMemorySize:
            return

        states, actions, rewards, states_, dones = self.memory.sample(self.batchSize)
        states  = torch.tensor(states , dtype=torch.float32).to(self.network.device)
        actions = torch.tensor(actions, dtype=torch.long   ).to(self.network.device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.network.device)
        states_ = torch.tensor(states_, dtype=torch.float32).to(self.network.device)
        dones   = torch.tensor(dones  , dtype=torch.bool   ).to(self.network.device)

        batchIndices = np.arange(self.batchSize, dtype=np.int64)
        qValue = self.network(states)[batchIndices, actions]

        qValues_ = self.network(states_)
        qValue_ = torch.max(qValues_, dim=1)[0]
        qValue_[dones] = 0.0

        td = reward + self.gamma * qValue_
        loss = self.network.loss(td, qValue)

        self.network.optimizer.zero_grad()
        loss.backward()
        self.network.optimizer.step()

        self.epsilon -= self.epsilon_decay
        if self.epsilon < self.epsilon_min:
            self.epsilon = self.epsilon_min

if __name__ == '__main__':
    env = gym.make('CartPole-v1').unwrapped
    agent = Agent(lr=0.001, inputShape=(4,), numActions=2)

    highScore = -math.inf
    episode = 0
    numSamples = 0
    while True:
        done = False
        state = env.reset()

        score, frame = 0, 1
        while not done:
            # env.render()

            action = agent.chooseAction(state)
            state_, reward, done, info = env.step(action)
            agent.storeMemory(state, action, reward, state_, done)
            agent.learn()
            
            state = state_

            numSamples += 1

            score += reward
            frame += 1

        highScore = max(highScore, score)

        print(( "ep {:4d}: high-score {:12.3f}, "
                "score {:12.3f}, epsilon {:5.3f}").format(
            episode, highScore, score, agent.epsilon))

        episode += 1</code></pre>
    <p>It's about 30 lines longer now, and there's a lot more that can go wrong.
      A few extra lines for epsilon-greedy and the rest for the
      experience replay. But, this prep should make the rest of the upgrade tutorials substantially
      easier, for me to write, and for you to read, and ultimately graduate from.
      Make sure to run the code to ensure it still gets some good scores.
      Play with the new settings to get a feel for how they affect performance.
    </p>
  </div>
  <div class="grid-container full">
    <h4>Moving Forward</h4>
    <p>
      Honestly this just isnt a very philosophical tutorial.
      It's time to move on to the dumptruck of upgrades awaiting your agent.<br>
      Good luck.
    </p>
    <a href="/index.html">Tutorial Index</a>
    <p></p>
    <p></p>
  </div>
</body>

</html>